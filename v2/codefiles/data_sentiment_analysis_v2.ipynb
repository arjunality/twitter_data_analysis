{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/arjunkhanchandani/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torchtext\n",
    "import tqdm\n",
    "import copy\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from flair.data import Sentence\n",
    "from flair.nn import Classifier\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentPoolEmbeddings\n",
    "tagger = Classifier.load('sentiment-fast')\n",
    "glove_embeddings = WordEmbeddings('glove')\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "from sklearn.cluster import *\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.decomposition import *\n",
    "from sklearn.feature_extraction import *\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.metrics import *\n",
    "from sklearn.metrics.pairwise import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18111, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>city</th>\n",
       "      <th>year</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>bad thing say government medical spare operati...</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2022</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>net family admitted government hospital privat...</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2022</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>vandi one hotspot measles well respiratory dis...</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2022</td>\n",
       "      <td>['#Measles']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>till medical negligence exist government hospi...</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2022</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>doctor reading also government hospital resident</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2022</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id                                              tweet    city  year  \\\n",
       "0         0  bad thing say government medical spare operati...  Mumbai  2022   \n",
       "1         1  net family admitted government hospital privat...  Mumbai  2022   \n",
       "2         2  vandi one hotspot measles well respiratory dis...  Mumbai  2022   \n",
       "3         3  till medical negligence exist government hospi...  Mumbai  2022   \n",
       "4         4   doctor reading also government hospital resident  Mumbai  2022   \n",
       "\n",
       "       hashtags  \n",
       "0            []  \n",
       "1            []  \n",
       "2  ['#Measles']  \n",
       "3            []  \n",
       "4            []  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df = pd.read_csv('/Users/arjunkhanchandani/Desktop/twitter_data_analysis/v2/data/tweets_cleaned_v2.csv')\n",
    "num_components = 2\n",
    "\n",
    "print(tweets_df.shape)\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>city</th>\n",
       "      <th>year</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>tweets_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>bad thing say government medical spare operati...</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2022</td>\n",
       "      <td>[]</td>\n",
       "      <td>[bad, thing, say, government, medical, spare, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>net family admitted government hospital privat...</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2022</td>\n",
       "      <td>[]</td>\n",
       "      <td>[net, family, admitted, government, hospital, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>vandi one hotspot measles well respiratory dis...</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2022</td>\n",
       "      <td>['#Measles']</td>\n",
       "      <td>[vandi, one, hotspot, measles, well, respirato...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>till medical negligence exist government hospi...</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2022</td>\n",
       "      <td>[]</td>\n",
       "      <td>[till, medical, negligence, exist, government,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>doctor reading also government hospital resident</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2022</td>\n",
       "      <td>[]</td>\n",
       "      <td>[doctor, reading, also, government, hospital, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id                                              tweet    city  year  \\\n",
       "0         0  bad thing say government medical spare operati...  Mumbai  2022   \n",
       "1         1  net family admitted government hospital privat...  Mumbai  2022   \n",
       "2         2  vandi one hotspot measles well respiratory dis...  Mumbai  2022   \n",
       "3         3  till medical negligence exist government hospi...  Mumbai  2022   \n",
       "4         4   doctor reading also government hospital resident  Mumbai  2022   \n",
       "\n",
       "       hashtags                                      tweets_tokens  \n",
       "0            []  [bad, thing, say, government, medical, spare, ...  \n",
       "1            []  [net, family, admitted, government, hospital, ...  \n",
       "2  ['#Measles']  [vandi, one, hotspot, measles, well, respirato...  \n",
       "3            []  [till, medical, negligence, exist, government,...  \n",
       "4            []  [doctor, reading, also, government, hospital, ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def creating_tokens(tweets):\n",
    "    tokenizer = TweetTokenizer()\n",
    "    tokens = tokenizer.tokenize(tweets)\n",
    "    return tokens\n",
    "\n",
    "tweets_df['tweets_tokens'] = tweets_df.apply(lambda x: creating_tokens(x['tweet']), axis=1)\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>city</th>\n",
       "      <th>year</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>tweets_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>bad thing say government medical spare operati...</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2022</td>\n",
       "      <td>[]</td>\n",
       "      <td>[bad, thing, say, government, medical, spare, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>net family admitted government hospital privat...</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2022</td>\n",
       "      <td>[]</td>\n",
       "      <td>[net, family, admitted, government, hospital, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>vandi one hotspot measles well respiratory dis...</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2022</td>\n",
       "      <td>['#Measles']</td>\n",
       "      <td>[vandi, one, hotspot, measles, well, respirato...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>till medical negligence exist government hospi...</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2022</td>\n",
       "      <td>[]</td>\n",
       "      <td>[till, medical, negligence, exist, government,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>doctor reading also government hospital resident</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2022</td>\n",
       "      <td>[]</td>\n",
       "      <td>[doctor, reading, also, government, hospital, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id                                              tweet    city  year  \\\n",
       "0         0  bad thing say government medical spare operati...  Mumbai  2022   \n",
       "1         1  net family admitted government hospital privat...  Mumbai  2022   \n",
       "2         2  vandi one hotspot measles well respiratory dis...  Mumbai  2022   \n",
       "3         3  till medical negligence exist government hospi...  Mumbai  2022   \n",
       "4         4   doctor reading also government hospital resident  Mumbai  2022   \n",
       "\n",
       "       hashtags                                      tweets_tokens  \n",
       "0            []  [bad, thing, say, government, medical, spare, ...  \n",
       "1            []  [net, family, admitted, government, hospital, ...  \n",
       "2  ['#Measles']  [vandi, one, hotspot, measles, well, respirato...  \n",
       "3            []  [till, medical, negligence, exist, government,...  \n",
       "4            []  [doctor, reading, also, government, hospital, ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_sentiment_df = tweets_df.copy()\n",
    "tweets_sentiment_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flair Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>city</th>\n",
       "      <th>year</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>tweets_tokens</th>\n",
       "      <th>sentiment_flair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>bad thing say government medical spare operati...</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2022</td>\n",
       "      <td>[]</td>\n",
       "      <td>[bad, thing, say, government, medical, spare, ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>net family admitted government hospital privat...</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2022</td>\n",
       "      <td>[]</td>\n",
       "      <td>[net, family, admitted, government, hospital, ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>vandi one hotspot measles well respiratory dis...</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2022</td>\n",
       "      <td>['#Measles']</td>\n",
       "      <td>[vandi, one, hotspot, measles, well, respirato...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>till medical negligence exist government hospi...</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2022</td>\n",
       "      <td>[]</td>\n",
       "      <td>[till, medical, negligence, exist, government,...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>doctor reading also government hospital resident</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2022</td>\n",
       "      <td>[]</td>\n",
       "      <td>[doctor, reading, also, government, hospital, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id                                              tweet    city  year  \\\n",
       "0         0  bad thing say government medical spare operati...  Mumbai  2022   \n",
       "1         1  net family admitted government hospital privat...  Mumbai  2022   \n",
       "2         2  vandi one hotspot measles well respiratory dis...  Mumbai  2022   \n",
       "3         3  till medical negligence exist government hospi...  Mumbai  2022   \n",
       "4         4   doctor reading also government hospital resident  Mumbai  2022   \n",
       "\n",
       "       hashtags                                      tweets_tokens  \\\n",
       "0            []  [bad, thing, say, government, medical, spare, ...   \n",
       "1            []  [net, family, admitted, government, hospital, ...   \n",
       "2  ['#Measles']  [vandi, one, hotspot, measles, well, respirato...   \n",
       "3            []  [till, medical, negligence, exist, government,...   \n",
       "4            []  [doctor, reading, also, government, hospital, ...   \n",
       "\n",
       "   sentiment_flair  \n",
       "0               -1  \n",
       "1               -1  \n",
       "2                1  \n",
       "3               -1  \n",
       "4                1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_flair_sentiment(tweets):\n",
    "    sentence = Sentence(tweets)\n",
    "    tagger.predict(sentence)\n",
    "    value = sentence.labels[0].to_dict()['value'] \n",
    "    if value == 'POSITIVE':\n",
    "        result = 1\n",
    "    else:\n",
    "        result = -1\n",
    "    return result\n",
    "\n",
    "tweets_sentiment_df['sentiment_flair'] = tweets_sentiment_df.apply(lambda x: get_flair_sentiment(x['tweet']), axis=1)\n",
    "tweets_sentiment_df.head()\n",
    "\n",
    "# x = get_flair_sentiment('this is not a movie')\n",
    "# x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nltk sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>city</th>\n",
       "      <th>year</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>tweets_tokens</th>\n",
       "      <th>sentiment_flair</th>\n",
       "      <th>sentiment_nltk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>bad thing say government medical spare operati...</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2022</td>\n",
       "      <td>[]</td>\n",
       "      <td>[bad, thing, say, government, medical, spare, ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>net family admitted government hospital privat...</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2022</td>\n",
       "      <td>[]</td>\n",
       "      <td>[net, family, admitted, government, hospital, ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>vandi one hotspot measles well respiratory dis...</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2022</td>\n",
       "      <td>['#Measles']</td>\n",
       "      <td>[vandi, one, hotspot, measles, well, respirato...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>till medical negligence exist government hospi...</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2022</td>\n",
       "      <td>[]</td>\n",
       "      <td>[till, medical, negligence, exist, government,...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>doctor reading also government hospital resident</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2022</td>\n",
       "      <td>[]</td>\n",
       "      <td>[doctor, reading, also, government, hospital, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id                                              tweet    city  year  \\\n",
       "0         0  bad thing say government medical spare operati...  Mumbai  2022   \n",
       "1         1  net family admitted government hospital privat...  Mumbai  2022   \n",
       "2         2  vandi one hotspot measles well respiratory dis...  Mumbai  2022   \n",
       "3         3  till medical negligence exist government hospi...  Mumbai  2022   \n",
       "4         4   doctor reading also government hospital resident  Mumbai  2022   \n",
       "\n",
       "       hashtags                                      tweets_tokens  \\\n",
       "0            []  [bad, thing, say, government, medical, spare, ...   \n",
       "1            []  [net, family, admitted, government, hospital, ...   \n",
       "2  ['#Measles']  [vandi, one, hotspot, measles, well, respirato...   \n",
       "3            []  [till, medical, negligence, exist, government,...   \n",
       "4            []  [doctor, reading, also, government, hospital, ...   \n",
       "\n",
       "   sentiment_flair  sentiment_nltk  \n",
       "0               -1              -1  \n",
       "1               -1               1  \n",
       "2                1              -1  \n",
       "3               -1              -1  \n",
       "4                1               1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_nltk_sentiment(tweet):\n",
    "    sia  = SentimentIntensityAnalyzer()\n",
    "    compound = sia.polarity_scores(tweet)['compound']\n",
    "    if compound >= 0:\n",
    "        sentiment = 1\n",
    "    else:\n",
    "        sentiment = -1\n",
    "    return sentiment\n",
    "\n",
    "tweets_sentiment_df['sentiment_nltk'] = tweets_sentiment_df.apply(lambda x: get_nltk_sentiment(x['tweet']), axis=1)\n",
    "tweets_sentiment_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "tfidf_embeddings = tfidf.fit_transform(tweets_sentiment_df['tweet'])\n",
    "tfidf_embeddings = np.asarray(tfidf_embeddings.todense())\n",
    "tfidf_embeddings = mm.fit_transform(tfidf_embeddings)\n",
    "\n",
    "#PCA\n",
    "tfidf_embeddings_pca = PCA(n_components=num_components).fit_transform(tfidf_embeddings)\n",
    "tfidf_embeddings_pca.shape\n",
    "\n",
    "#NMF\n",
    "nmf = NMF(n_components=num_components)\n",
    "tfidf_embeddings_nmf = nmf.fit_transform(tfidf_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_tfidf = KMeans(n_clusters=2, max_iter=100, random_state=42).fit(X=tfidf_embeddings_pca)\n",
    "positive_cluster_center = kmeans_tfidf.cluster_centers_[0]\n",
    "negative_cluster_center = kmeans_tfidf.cluster_centers_[1]\n",
    "\n",
    "tfidf_labels = kmeans_tfidf.predict(tfidf_embeddings_pca)\n",
    "plt.scatter(tfidf_embeddings_pca[:,0], tfidf_embeddings_pca[:,1], c=kmeans_tfidf.labels_, cmap='viridis', s=5)\n",
    "print(Counter(tfidf_labels))\n",
    "print(tfidf_labels)\n",
    "tweets_sentiment_df['sentiment_tfidf_kmeans_pca'] = tfidf_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ward \n",
    "agg_ward_tfidf = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward')\n",
    "agg_ward_tfidf.fit(X=tfidf_embeddings_pca)\n",
    "plt.scatter(tfidf_embeddings_pca[:,0], tfidf_embeddings_pca[:,1], c=agg_ward_tfidf.labels_, cmap='viridis', s=5)\n",
    "\n",
    "tfidf_labels = agg_ward_tfidf.labels_\n",
    "print(Counter(tfidf_labels))\n",
    "print(tfidf_labels)\n",
    "tweets_sentiment_df['sentiment_tfidf_agg_ward_pca'] = tfidf_labels\n",
    "\n",
    "#complete\n",
    "agg_complete_tfidf = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='complete')\n",
    "agg_complete_tfidf.fit(X=tfidf_embeddings_pca)\n",
    "plt.scatter(tfidf_embeddings_pca[:,0], tfidf_embeddings_pca[:,1], c=agg_complete_tfidf.labels_, cmap='viridis', s=5)\n",
    "\n",
    "tfidf_labels = agg_complete_tfidf.labels_\n",
    "print(Counter(tfidf_labels))\n",
    "print(tfidf_labels)\n",
    "tweets_sentiment_df['sentiment_tfidf_agg_complete_pca'] = tfidf_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Birch Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birch_tfidf = Birch(n_clusters=2, threshold=0.000001, branching_factor=5)\n",
    "birch_tfidf.fit(tfidf_embeddings_pca)\n",
    "plt.scatter(tfidf_embeddings_pca[:,0], tfidf_embeddings_pca[:,1], c=birch_tfidf.labels_, cmap='viridis', s=5)\n",
    "\n",
    "tfidf_labels = birch_tfidf.labels_\n",
    "print(Counter(tfidf_labels))\n",
    "print(tfidf_labels)\n",
    "tweets_sentiment_df['sentiment_tfidf_birch_pca'] = tfidf_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_tfidf = KMeans(n_clusters=2, max_iter=100, random_state=42).fit(X=tfidf_embeddings_nmf)\n",
    "positive_cluster_center = kmeans_tfidf.cluster_centers_[0]\n",
    "negative_cluster_center = kmeans_tfidf.cluster_centers_[1]\n",
    "\n",
    "tfidf_labels = kmeans_tfidf.predict(tfidf_embeddings_nmf)\n",
    "plt.scatter(tfidf_embeddings_nmf[:,0], tfidf_embeddings_nmf[:,1], c=kmeans_tfidf.labels_, cmap='viridis', s=5)\n",
    "print(Counter(tfidf_labels))\n",
    "print(tfidf_labels)\n",
    "tweets_sentiment_df['sentiment_tfidf_kmeans_mnf'] = tfidf_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agglomerative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ward \n",
    "agg_ward_tfidf = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward')\n",
    "agg_ward_tfidf.fit(X=tfidf_embeddings_nmf)\n",
    "plt.scatter(tfidf_embeddings_nmf[:,0], tfidf_embeddings_nmf[:,1], c=agg_ward_tfidf.labels_, cmap='viridis', s=5)\n",
    "plt.show()\n",
    "tfidf_labels = agg_ward_tfidf.labels_\n",
    "print(Counter(tfidf_labels))\n",
    "print(tfidf_labels)\n",
    "tweets_sentiment_df['sentiment_tfidf_agg_ward_nmf'] = tfidf_labels\n",
    "\n",
    "#complete\n",
    "agg_complete_tfidf = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='complete')\n",
    "agg_complete_tfidf.fit(X=tfidf_embeddings_nmf)\n",
    "plt.scatter(tfidf_embeddings_nmf[:,0], tfidf_embeddings_nmf[:,1], c=agg_complete_tfidf.labels_, cmap='viridis', s=5)\n",
    "plt.show()\n",
    "tfidf_labels = agg_complete_tfidf.labels_\n",
    "print(Counter(tfidf_labels))\n",
    "print(tfidf_labels)\n",
    "tweets_sentiment_df['sentiment_tfidf_agg_complete_nmf'] = tfidf_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Birch Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birch_tfidf = Birch(n_clusters=2, threshold=0.00001, branching_factor=50)\n",
    "birch_tfidf.fit(tfidf_embeddings_nmf)\n",
    "plt.scatter(tfidf_embeddings_nmf[:,0], tfidf_embeddings_nmf[:,1], c=birch_tfidf.labels_, cmap='viridis', s=5)\n",
    "\n",
    "tfidf_labels = birch_tfidf.labels_\n",
    "print(Counter(tfidf_labels))\n",
    "print(tfidf_labels)\n",
    "tweets_sentiment_df['sentiment_tfidf_birch_nmf'] = tfidf_labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 200\n",
    "window_size = 5\n",
    "min_word = 2\n",
    "down_sampling = 1e-2\n",
    "tokens = tweets_sentiment_df['tweets_tokens']\n",
    "\n",
    "\n",
    "fast_text_model = FastText(\n",
    "                        sample=down_sampling,\n",
    "                        workers = 4,\n",
    "                        sg=1\n",
    "                    )\n",
    "\n",
    "fast_text_model.build_vocab(tokens)\n",
    "fast_text_model.train(tokens, total_examples=fast_text_model.corpus_count, epochs=fast_text_model.epochs)\n",
    "\n",
    "fasttext_embeddings = np.array([np.mean([fast_text_model.wv[word] for word in token], axis=0) for token in tokens])\n",
    "fasttext_embeddings.shape\n",
    "\n",
    "# fasttext_embeddings = np.asarray(fasttext_embeddings.todense())\n",
    "fasttext_embeddings = mm.fit_transform(fasttext_embeddings)\n",
    "\n",
    "#PCA\n",
    "# fasttext_embeddings = np.asarray(fasttext_embeddings)\n",
    "fasttext_embeddings_pca = PCA(n_components=num_components).fit_transform(fasttext_embeddings)\n",
    "print(fasttext_embeddings_pca.shape)\n",
    "\n",
    "# #NMF\n",
    "nmf = NMF(n_components=num_components)\n",
    "fasttext_embeddings_nmf = nmf.fit_transform(fasttext_embeddings)\n",
    "print(fasttext_embeddings_nmf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_fasttext = KMeans(n_clusters=2, max_iter=100, random_state=42).fit(X=fasttext_embeddings_pca)\n",
    "positive_cluster_center = kmeans_fasttext.cluster_centers_[0]\n",
    "negative_cluster_center = kmeans_fasttext.cluster_centers_[1]\n",
    "\n",
    "fasttext_labels = kmeans_fasttext.predict(fasttext_embeddings_pca)\n",
    "plt.scatter(fasttext_embeddings_pca[:,0], fasttext_embeddings_pca[:,1], c=kmeans_fasttext.labels_, cmap='viridis', s=5)\n",
    "print(Counter(fasttext_labels))\n",
    "print(fasttext_labels)\n",
    "tweets_sentiment_df['sentiment_fasttext_kmeans_pca'] = fasttext_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ward \n",
    "agg_ward_fasttext = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward')\n",
    "agg_ward_fasttext.fit(X=fasttext_embeddings_pca)\n",
    "fasttext_labels = agg_ward_fasttext.labels_\n",
    "plt.scatter(fasttext_embeddings_pca[:,0], fasttext_embeddings_pca[:,1], c=agg_ward_fasttext.labels_, cmap='viridis', s=5)\n",
    "plt.show()\n",
    "print(Counter(fasttext_labels))\n",
    "print(fasttext_labels)\n",
    "tweets_sentiment_df['sentiment_fasttext_agg_ward_pca'] = fasttext_labels\n",
    "\n",
    "# #complete\n",
    "agg_complete_fasttext = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='complete')\n",
    "agg_complete_fasttext.fit(X=fasttext_embeddings_pca)\n",
    "fasttext_labels = agg_complete_fasttext.labels_\n",
    "plt.scatter(fasttext_embeddings_pca[:,0], fasttext_embeddings_pca[:,1], c=agg_complete_fasttext.labels_, cmap='viridis', s=5)\n",
    "plt.show()\n",
    "print(Counter(fasttext_labels))\n",
    "print(fasttext_labels)\n",
    "tweets_sentiment_df['sentiment_fasttext_agg_complete_pca'] = fasttext_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Birch Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birch_fasttext = Birch(n_clusters=2, threshold=0.00001, branching_factor=50)\n",
    "birch_fasttext.fit(fasttext_embeddings_pca)\n",
    "plt.scatter(fasttext_embeddings_pca[:,0], fasttext_embeddings_pca[:,1], c=birch_fasttext.labels_, cmap='viridis', s=5)\n",
    "\n",
    "fasttext_labels = birch_fasttext.labels_\n",
    "print(Counter(fasttext_labels))\n",
    "print(fasttext_labels)\n",
    "tweets_sentiment_df['sentiment_fasttext_birch_pca'] = fasttext_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_fasttext = KMeans(n_clusters=2, max_iter=100, random_state=42).fit(X=fasttext_embeddings_nmf)\n",
    "positive_cluster_center = kmeans_fasttext.cluster_centers_[0]\n",
    "negative_cluster_center = kmeans_fasttext.cluster_centers_[1]\n",
    "\n",
    "fasttext_labels = kmeans_fasttext.predict(fasttext_embeddings_nmf)\n",
    "plt.scatter(fasttext_embeddings_nmf[:,0], fasttext_embeddings_nmf[:,1], c=kmeans_fasttext.labels_, cmap='viridis', s=5)\n",
    "print(Counter(fasttext_labels))\n",
    "print(fasttext_labels)\n",
    "tweets_sentiment_df['sentiment_fasttext_kmeans_nmf'] = fasttext_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ward \n",
    "agg_ward_fasttext = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward')\n",
    "agg_ward_fasttext.fit(X=fasttext_embeddings_nmf)\n",
    "fasttext_labels = agg_ward_fasttext.labels_\n",
    "plt.scatter(fasttext_embeddings_nmf[:,0], fasttext_embeddings_nmf[:,1], c=agg_ward_fasttext.labels_, cmap='viridis', s=5)\n",
    "plt.show()\n",
    "print(Counter(fasttext_labels))\n",
    "print(fasttext_labels)\n",
    "tweets_sentiment_df['sentiment_fasttext_agg_ward_nmf'] = fasttext_labels\n",
    "\n",
    "# #complete\n",
    "agg_complete_fasttext = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='complete')\n",
    "agg_complete_fasttext.fit(X=fasttext_embeddings_nmf)\n",
    "fasttext_labels = agg_complete_fasttext.labels_\n",
    "plt.scatter(fasttext_embeddings_nmf[:,0], fasttext_embeddings_nmf[:,1], c=agg_complete_fasttext.labels_, cmap='viridis', s=5)\n",
    "plt.show()\n",
    "print(Counter(fasttext_labels))\n",
    "print(fasttext_labels)\n",
    "tweets_sentiment_df['sentiment_fasttext_agg_complete_nmf'] = fasttext_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Birch Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birch_fasttext = Birch(n_clusters=2, threshold=0.00001, branching_factor=50)\n",
    "birch_fasttext.fit(fasttext_embeddings_nmf)\n",
    "plt.scatter(fasttext_embeddings_nmf[:,0], fasttext_embeddings_nmf[:,1], c=birch_fasttext.labels_, cmap='viridis', s=5)\n",
    "\n",
    "fasttext_labels = birch_fasttext.labels_\n",
    "print(Counter(fasttext_labels))\n",
    "print(fasttext_labels)\n",
    "tweets_sentiment_df['sentiment_fasttext_birch_nmf'] = fasttext_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_tokens = tweets_sentiment_df['tweets_tokens']\n",
    "\n",
    "#Detecting Common Phrases so each word is treated as its own\n",
    "tweets_phrases = gensim.models.phrases.Phrases(tweets_tokens)\n",
    "tweets_phraser = gensim.models.phrases.Phraser(tweets_phrases)\n",
    "tweets_phrase = tweets_phraser[tweets_tokens]\n",
    "\n",
    "MODEL_TRAIN = gensim.models.word2vec.Word2Vec(sentences=tweets_phrase, workers=2)\n",
    "\n",
    "def word2vec(tokens, model, text_input):\n",
    "    embeddings = []\n",
    "    for row in text_input:\n",
    "        row_vector = np.zeros(model.vector_size)\n",
    "        for word in row:\n",
    "            if word in model.wv.index_to_key:\n",
    "                word_vector = model.wv[word]\n",
    "                row_vector += word_vector\n",
    "        embeddings.append(row_vector)\n",
    "    return np.array(embeddings)\n",
    "\n",
    "word2vec_embeddings = word2vec(tweets_phrase, MODEL_TRAIN, tweets_phrase)\n",
    "word2vec_embeddings.shape\n",
    "\n",
    "word2vec_embeddings = mm.fit_transform(word2vec_embeddings)\n",
    "\n",
    "#PCA\n",
    "# word2vec_embeddings = np.asarray(word2vec_embeddings)\n",
    "word2vec_embeddings_pca = PCA(n_components=num_components).fit_transform(word2vec_embeddings)\n",
    "print(word2vec_embeddings_pca.shape)\n",
    "\n",
    "# #NMF\n",
    "nmf = NMF(n_components=num_components)\n",
    "word2vec_embeddings_nmf = nmf.fit_transform(word2vec_embeddings)\n",
    "print(word2vec_embeddings_nmf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_word2vec = KMeans(n_clusters=2, max_iter=100, random_state=42).fit(X=word2vec_embeddings_pca)\n",
    "positive_cluster_center = kmeans_word2vec.cluster_centers_[0]\n",
    "negative_cluster_center = kmeans_word2vec.cluster_centers_[1]\n",
    "\n",
    "word2vec_labels = kmeans_word2vec.predict(word2vec_embeddings_pca)\n",
    "plt.scatter(word2vec_embeddings_pca[:,0], word2vec_embeddings_pca[:,1], c=kmeans_word2vec.labels_, cmap='viridis', s=5)\n",
    "print(Counter(word2vec_labels))\n",
    "print(word2vec_labels)\n",
    "tweets_sentiment_df['sentiment_word2vec_kmeans_pca'] = word2vec_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ward \n",
    "agg_ward_word2vec = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward')\n",
    "agg_ward_word2vec.fit(X=word2vec_embeddings_pca)\n",
    "word2vec_labels = agg_ward_word2vec.labels_\n",
    "plt.scatter(word2vec_embeddings_pca[:,0], word2vec_embeddings_pca[:,1], c=agg_ward_word2vec.labels_, cmap='viridis', s=1)\n",
    "plt.show()\n",
    "print(Counter(word2vec_labels))\n",
    "print(word2vec_labels)\n",
    "tweets_sentiment_df['sentiment_word2vec_agg_ward_pca'] = word2vec_labels\n",
    "\n",
    "#complete\n",
    "agg_complete_word2vec = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='complete')\n",
    "agg_complete_word2vec.fit(X=word2vec_embeddings_pca)\n",
    "word2vec_labels = agg_complete_word2vec.labels_\n",
    "plt.scatter(word2vec_embeddings_pca[:,0], word2vec_embeddings_pca[:,1], c=agg_complete_word2vec.labels_, cmap='viridis', s=1)\n",
    "plt.show()\n",
    "print(Counter(word2vec_labels))\n",
    "print(word2vec_labels)\n",
    "tweets_sentiment_df['sentiment_word2vec_agg_complete_pca'] = word2vec_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Birch Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birch_word2vec = Birch(n_clusters=2, threshold=0.00001, branching_factor=50)\n",
    "birch_word2vec.fit(word2vec_embeddings_pca)\n",
    "plt.scatter(word2vec_embeddings_pca[:,0], word2vec_embeddings_pca[:,1], c=birch_word2vec.labels_, cmap='viridis', s=5)\n",
    "\n",
    "word2vec_labels = birch_word2vec.labels_\n",
    "print(Counter(word2vec_labels))\n",
    "print(word2vec_labels)\n",
    "tweets_sentiment_df['sentiment_word2vec_birch_pca'] = word2vec_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_word2vec = KMeans(n_clusters=2, max_iter=100, random_state=42).fit(X=word2vec_embeddings_nmf)\n",
    "positive_cluster_center = kmeans_word2vec.cluster_centers_[0]\n",
    "negative_cluster_center = kmeans_word2vec.cluster_centers_[1]\n",
    "\n",
    "word2vec_labels = kmeans_word2vec.predict(word2vec_embeddings_nmf)\n",
    "plt.scatter(word2vec_embeddings_nmf[:,0], word2vec_embeddings_nmf[:,1], c=kmeans_word2vec.labels_, cmap='viridis', s=5)\n",
    "print(Counter(word2vec_labels))\n",
    "print(word2vec_labels)\n",
    "tweets_sentiment_df['sentiment_word2vec_kmeans_nmf'] = word2vec_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ward \n",
    "agg_ward_word2vec = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward')\n",
    "agg_ward_word2vec.fit(X=word2vec_embeddings_nmf)\n",
    "word2vec_labels = agg_ward_word2vec.labels_\n",
    "plt.scatter(word2vec_embeddings_nmf[:,0], word2vec_embeddings_nmf[:,1], c=agg_ward_word2vec.labels_, cmap='viridis', s=1)\n",
    "plt.show()\n",
    "print(Counter(word2vec_labels))\n",
    "print(word2vec_labels)\n",
    "tweets_sentiment_df['sentiment_word2vec_agg_ward_nmf'] = word2vec_labels\n",
    "\n",
    "#complete\n",
    "agg_complete_word2vec = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='complete')\n",
    "agg_complete_word2vec.fit(X=word2vec_embeddings_nmf)\n",
    "word2vec_labels = agg_complete_word2vec.labels_\n",
    "plt.scatter(word2vec_embeddings_nmf[:,0], word2vec_embeddings_nmf[:,1], c=agg_complete_word2vec.labels_, cmap='viridis', s=1)\n",
    "plt.show()\n",
    "print(Counter(word2vec_labels))\n",
    "print(word2vec_labels)\n",
    "tweets_sentiment_df['sentiment_word2vec_agg_complete_nmf'] = word2vec_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Birch Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birch_word2vec = Birch(n_clusters=2, threshold=0.00001, branching_factor=50)\n",
    "birch_word2vec.fit(word2vec_embeddings_nmf)\n",
    "plt.scatter(word2vec_embeddings_nmf[:,0], word2vec_embeddings_nmf[:,1], c=birch_word2vec.labels_, cmap='viridis', s=5)\n",
    "\n",
    "word2vec_labels = birch_word2vec.labels_\n",
    "print(Counter(word2vec_labels))\n",
    "print(word2vec_labels)\n",
    "tweets_sentiment_df['sentiment_word2vec_birch_nmf'] = word2vec_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = torchtext.vocab.GloVe(name=\"6B\", dim=200, max_vectors=10000)\n",
    "def split_text(text):\n",
    "    return text.split()\n",
    "\n",
    "def get_glove_embeddings(glove_vector, x_train_input):\n",
    "    train = []\n",
    "    for line in enumerate(x_train_input):\n",
    "        text = line[-1]\n",
    "        vector_sum = sum(glove_vector[w] for w in split_text(text))\n",
    "        label = torch.tensor(int(line[0] == \"4\")).long()\n",
    "        train.append((vector_sum, label))\n",
    "            \n",
    "    return train\n",
    "\n",
    "glove_embeddings = get_glove_embeddings(glove, tweets_sentiment_df['tweet'])\n",
    "glove_embeddings = np.array([x[0].numpy() for x in glove_embeddings])\n",
    "\n",
    "glove_embeddings = mm.fit_transform(glove_embeddings)\n",
    "\n",
    "#PCA\n",
    "glove_embeddings_pca = PCA(n_components=num_components).fit_transform(glove_embeddings)\n",
    "print(glove_embeddings_pca.shape)\n",
    "\n",
    "# #NMF\n",
    "nmf = NMF(n_components=num_components)\n",
    "glove_embeddings_nmf = nmf.fit_transform(glove_embeddings)\n",
    "print(glove_embeddings_nmf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_glove = KMeans(n_clusters=2, max_iter=100, random_state=42).fit(X=glove_embeddings_pca)\n",
    "positive_cluster_center = kmeans_glove.cluster_centers_[0]\n",
    "negative_cluster_center = kmeans_glove.cluster_centers_[1]\n",
    "\n",
    "glove_labels = kmeans_glove.predict(glove_embeddings_pca)\n",
    "plt.scatter(glove_embeddings_pca[:,0], glove_embeddings_pca[:,1], c=kmeans_glove.labels_, cmap='viridis', s=5)\n",
    "print(Counter(glove_labels))\n",
    "print(glove_labels)\n",
    "tweets_sentiment_df['sentiment_glove_kmeans_pca'] = glove_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ward \n",
    "agg_ward_glove = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward')\n",
    "agg_ward_glove.fit(X=glove_embeddings_pca)\n",
    "glove_labels = agg_ward_glove.labels_\n",
    "plt.scatter(glove_embeddings_pca[:,0], glove_embeddings_pca[:,1], c=agg_ward_glove.labels_, cmap='viridis', s=1)\n",
    "plt.show()\n",
    "print(Counter(glove_labels))\n",
    "print(glove_labels)\n",
    "tweets_sentiment_df['sentiment_glove_agg_ward_pca'] = glove_labels\n",
    "\n",
    "#complete\n",
    "agg_complete_glove = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='complete')\n",
    "agg_complete_glove.fit(X=glove_embeddings_pca)\n",
    "glove_labels = agg_complete_glove.labels_\n",
    "plt.scatter(glove_embeddings_pca[:,0], glove_embeddings_pca[:,1], c=agg_complete_glove.labels_, cmap='viridis', s=1)\n",
    "plt.show()\n",
    "print(Counter(glove_labels))\n",
    "print(glove_labels)\n",
    "tweets_sentiment_df['sentiment_glove_agg_complete_pca'] = glove_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Birch Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birch_glove = Birch(n_clusters=2, threshold=0.00001, branching_factor=50)\n",
    "birch_glove.fit(glove_embeddings_pca)\n",
    "plt.scatter(glove_embeddings_pca[:,0], glove_embeddings_pca[:,1], c=birch_glove.labels_, cmap='viridis', s=5)\n",
    "\n",
    "glove_labels = birch_glove.labels_\n",
    "print(Counter(glove_labels))\n",
    "print(glove_labels)\n",
    "tweets_sentiment_df['sentiment_glove_birch_pca'] = glove_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_glove = KMeans(n_clusters=2, max_iter=100, random_state=42).fit(X=glove_embeddings_nmf)\n",
    "positive_cluster_center = kmeans_glove.cluster_centers_[0]\n",
    "negative_cluster_center = kmeans_glove.cluster_centers_[1]\n",
    "\n",
    "glove_labels = kmeans_glove.predict(glove_embeddings_nmf)\n",
    "plt.scatter(glove_embeddings_nmf[:,0], glove_embeddings_nmf[:,1], c=kmeans_glove.labels_, cmap='viridis', s=5)\n",
    "print(Counter(glove_labels))\n",
    "print(glove_labels)\n",
    "tweets_sentiment_df['sentiment_glove_kmeans_nmf'] = glove_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ward \n",
    "agg_ward_glove = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward')\n",
    "agg_ward_glove.fit(X=glove_embeddings_nmf)\n",
    "glove_labels = agg_ward_glove.labels_\n",
    "plt.scatter(glove_embeddings_nmf[:,0], glove_embeddings_nmf[:,1], c=agg_ward_glove.labels_, cmap='viridis', s=1)\n",
    "plt.show()\n",
    "print(Counter(glove_labels))\n",
    "print(glove_labels)\n",
    "tweets_sentiment_df['sentiment_glove_agg_ward_nmf'] = glove_labels\n",
    "\n",
    "#complete\n",
    "agg_complete_glove = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='complete')\n",
    "agg_complete_glove.fit(X=glove_embeddings_nmf)\n",
    "glove_labels = agg_complete_glove.labels_\n",
    "plt.scatter(glove_embeddings_nmf[:,0], glove_embeddings_nmf[:,1], c=agg_complete_glove.labels_, cmap='viridis', s=1)\n",
    "plt.show()\n",
    "print(Counter(glove_labels))\n",
    "print(glove_labels)\n",
    "tweets_sentiment_df['sentiment_glove_agg_complete_nmf'] = glove_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Birch Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birch_glove = Birch(n_clusters=2, threshold=0.00001, branching_factor=50)\n",
    "birch_glove.fit(glove_embeddings_nmf)\n",
    "plt.scatter(glove_embeddings_nmf[:,0], glove_embeddings_nmf[:,1], c=birch_glove.labels_, cmap='viridis', s=5)\n",
    "\n",
    "glove_labels = birch_glove.labels_\n",
    "print(Counter(glove_labels))\n",
    "print(glove_labels)\n",
    "tweets_sentiment_df['sentiment_glove_birch_nmf'] = glove_labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweets_sentiment_df.sentiment_flair.value_counts())\n",
    "print(tweets_sentiment_df.sentiment_nltk.value_counts())\n",
    "print(tweets_sentiment_df.sentiment_flair_glove_embed_kmeans.value_counts())\n",
    "# print(tweets_sentiment_df.sentiment_flair_document_embed_kmeans.value_counts())\n",
    "print(tweets_sentiment_df.sentiment_fast_text_embed_kmeans.value_counts())\n",
    "print(tweets_sentiment_df.sentiment_fast_text_embed_nmf_kmeans.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.to_csv(tweets_sentiment_df, '/Users/arjunkhanchandani/Desktop/twitter_data_analysis-main/v2/data/tweets_with_sentiment__version_2_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pub.towardsai.net/sentiment-analysis-without-modeling-textblob-vs-vader-vs-flair-657b7af855f4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Twitter_Data_Analysis-wZ120kVj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
