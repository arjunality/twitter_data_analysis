{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/nitanshjain/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/nitanshjain/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/nitanshjain/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "import fnmatch\n",
    "import string\n",
    "from urllib.parse import urlparse\n",
    "import contractions\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('words')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, TweetTokenizer\n",
    "from nltk.corpus import words, stopwords\n",
    "from nltk.metrics.distance import jaccard_distance, edit_distance\n",
    "from nltk.util import ngrams\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from deep_translator import GoogleTranslator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26015, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>date_created</th>\n",
       "      <th>tweet</th>\n",
       "      <th>city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2022-12-12 16:13:45+00:00</td>\n",
       "      <td>@esichq @byadavbjp @Rameswar_Teli @mygovindia ...</td>\n",
       "      <td>Mumbai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2022-12-10 06:30:56+00:00</td>\n",
       "      <td>@PotholeWarriors @CMOMaharashtra @mieknathshin...</td>\n",
       "      <td>Mumbai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2022-11-23 13:09:18+00:00</td>\n",
       "      <td>@Iam_Ayushmann Govandi is one of the Hotspot o...</td>\n",
       "      <td>Mumbai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2022-10-27 15:58:11+00:00</td>\n",
       "      <td>Till when medical negligence will exist in gov...</td>\n",
       "      <td>Mumbai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2022-07-28 03:03:15+00:00</td>\n",
       "      <td>Me being a doctor reading this\\nAlso governmen...</td>\n",
       "      <td>Mumbai</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0               date_created  \\\n",
       "0           0  2022-12-12 16:13:45+00:00   \n",
       "1           1  2022-12-10 06:30:56+00:00   \n",
       "2           2  2022-11-23 13:09:18+00:00   \n",
       "3           3  2022-10-27 15:58:11+00:00   \n",
       "4           4  2022-07-28 03:03:15+00:00   \n",
       "\n",
       "                                               tweet    city  \n",
       "0  @esichq @byadavbjp @Rameswar_Teli @mygovindia ...  Mumbai  \n",
       "1  @PotholeWarriors @CMOMaharashtra @mieknathshin...  Mumbai  \n",
       "2  @Iam_Ayushmann Govandi is one of the Hotspot o...  Mumbai  \n",
       "3  Till when medical negligence will exist in gov...  Mumbai  \n",
       "4  Me being a doctor reading this\\nAlso governmen...  Mumbai  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df = pd.read_csv('/Users/nitanshjain/Documents/Projects/Twitter_Data_Analysis/v2/data/tweets_v2.csv')\n",
    "print(tweets_df.shape)\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet_id         int64\n",
      "date_created    object\n",
      "tweet           object\n",
      "city            object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "tweets_df.rename(columns = {'Unnamed: 0':'tweet_id'}, inplace=True)\n",
    "print(tweets_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>city</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@esichq @byadavbjp @Rameswar_Teli @mygovindia ...</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>@PotholeWarriors @CMOMaharashtra @mieknathshin...</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>@Iam_Ayushmann Govandi is one of the Hotspot o...</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Till when medical negligence will exist in gov...</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Me being a doctor reading this\\nAlso governmen...</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id                                              tweet    city  year\n",
       "0         0  @esichq @byadavbjp @Rameswar_Teli @mygovindia ...  Mumbai  2022\n",
       "1         1  @PotholeWarriors @CMOMaharashtra @mieknathshin...  Mumbai  2022\n",
       "2         2  @Iam_Ayushmann Govandi is one of the Hotspot o...  Mumbai  2022\n",
       "3         3  Till when medical negligence will exist in gov...  Mumbai  2022\n",
       "4         4  Me being a doctor reading this\\nAlso governmen...  Mumbai  2022"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#selecting only year from date_created column\n",
    "tweets_df['date_created'] = pd.to_datetime(tweets_df['date_created'])\n",
    "tweets_df['year'] = tweets_df['date_created'].dt.year\n",
    "tweets_df.drop(['date_created'], axis=1, inplace=True)\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delhi        10005\n",
      "Mumbai        6715\n",
      "Hyderabad     3597\n",
      "Bangalore     3229\n",
      "Kolkata       1413\n",
      "Chennai       1056\n",
      "Name: city, dtype: int64\n",
      "2020    8963\n",
      "2021    7255\n",
      "2019    3885\n",
      "2022    3214\n",
      "2018    2698\n",
      "Name: year, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(tweets_df.city.value_counts())\n",
    "print(tweets_df.year.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset before removal of duplicates is (26015, 4)\n",
      "Shape of dataset after removal of duplicates is (19350, 4)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of dataset before removal of duplicates is {}'.format(tweets_df.shape))\n",
    "tweets_no_dupl_df = tweets_df.drop_duplicates(subset=['tweet'])\n",
    "print('Shape of dataset after removal of duplicates is {}'.format(tweets_no_dupl_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delhi        7379\n",
      "Mumbai       4963\n",
      "Hyderabad    2614\n",
      "Bangalore    2471\n",
      "Kolkata      1077\n",
      "Chennai       846\n",
      "Name: city, dtype: int64\n",
      "2020    6517\n",
      "2021    5515\n",
      "2019    2767\n",
      "2022    2454\n",
      "2018    2097\n",
      "Name: year, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(tweets_no_dupl_df.city.value_counts())\n",
    "print(tweets_no_dupl_df.year.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def removing_links(df):\n",
    "     for tweets in df.loc[:,'tweet']:\n",
    "          \n",
    "          tokenizer = TweetTokenizer()\n",
    "          tweet_id = df.loc[df['tweet'] == tweets, 'tweet_id'].values[0]\n",
    "        \n",
    "          tokens = tokenizer.tokenize(tweets)\n",
    "          x = [word for word in tokens if not urlparse(word).scheme]\n",
    "          tweets = ' '.join(x)\n",
    "          \n",
    "          df.loc[df['tweet_id']==tweet_id, 'tweet'] = tweets\n",
    "          \n",
    "     return df\n",
    "\n",
    "def contractions_handling(df):\n",
    "     for tweets in df.loc[:,'tweet']:\n",
    "          \n",
    "          tweet_id = df.loc[df['tweet'] == tweets, 'tweet_id'].values[0]\n",
    "          tweets = contractions.fix(tweets)\n",
    "          \n",
    "          df.loc[df['tweet_id']==tweet_id, 'tweet'] = tweets\n",
    "          \n",
    "     return df\n",
    "\n",
    "def adding_space_bw_words_punc(df):\n",
    "     for tweets in df.loc[:, 'tweet']:\n",
    "          \n",
    "          tokenizer = TweetTokenizer()\n",
    "          tweet_id = df.loc[df['tweet'] == tweets, 'tweet_id'].values[0]\n",
    "          tweets = tweets.replace(',', ' , ').replace('.', ' . ').replace('?', ' ? ').replace('!', ' ! ')\n",
    "          \n",
    "          df.loc[df['tweet_id']==tweet_id, 'tweet'] = tweets\n",
    "     \n",
    "     return df\n",
    "\n",
    "def removing_hashtags_mentions(df):\n",
    "     for tweets in df.loc[:, 'tweet']:\n",
    "          \n",
    "          tokenizer = TweetTokenizer()\n",
    "          tweet_id = df.loc[df['tweet'] == tweets, 'tweet_id'].values[0]\n",
    "          \n",
    "          tokens = tokenizer.tokenize(tweets)\n",
    "          tokens = [word for word in tokens if word[0] not in ('#', '@')]\n",
    "          tokens = [word for word in tokens if word[0] not in ('â–ª')]\n",
    "          tweets = ' '.join(tokens)\n",
    "          \n",
    "          df.loc[df['tweet_id']==tweet_id, 'tweet'] = tweets\n",
    "     \n",
    "     return df\n",
    "          \n",
    "def removing_punctuations_emojis(df):\n",
    "     for tweets in df.loc[:, 'tweet']:\n",
    "          \n",
    "          tokenizer = TweetTokenizer()\n",
    "          tweet_id = df.loc[df['tweet'] == tweets, 'tweet_id'].values[0]\n",
    "          \n",
    "          tweets = tweets.translate(str.maketrans('', '', string.punctuation)) # removes punctuations\n",
    "          tweets = re.sub(r'[^\\x00-\\x7F]+', ' ', tweets) # removes emojis\n",
    "          tweets = tweets.lower() # converts text to lower case\n",
    "          \n",
    "          df.loc[df['tweet_id']==tweet_id, 'tweet'] = tweets\n",
    "     \n",
    "     return df\n",
    "\n",
    "def removing_numbers(df):\n",
    "     for tweets in df.loc[:, 'tweet']:\n",
    "          \n",
    "          tokenizer = TweetTokenizer()\n",
    "          tweet_id = df.loc[df['tweet'] == tweets, 'tweet_id'].values[0]\n",
    "          \n",
    "          #remove numbers\n",
    "          tweets = re.sub(r'\\d+', '', tweets)\n",
    "          tweets = re.sub(' +', ' ', tweets)\n",
    "          \n",
    "          df.loc[df['tweet_id']==tweet_id, 'tweet'] = tweets\n",
    "     \n",
    "     return df\n",
    "\n",
    "def removing_stopwords(df):\n",
    "     for tweets in df.loc[:,'tweet']:\n",
    "          \n",
    "          tokenizer = TweetTokenizer()\n",
    "          tweet_id = df.loc[df['tweet'] == tweets, 'tweet_id'].values[0]\n",
    "          \n",
    "          tokens = tokenizer.tokenize(tweets)\n",
    "          filtered_tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "          tweets = ' '.join(filtered_tokens)\n",
    "          \n",
    "          df.loc[df['tweet_id']==tweet_id, 'tweet'] = tweets\n",
    "          \n",
    "     return df\n",
    "\n",
    "def lemmatization(df):\n",
    "     for tweets in df.loc[:,'tweet']:\n",
    "          \n",
    "          tokenizer = TweetTokenizer()\n",
    "          tweet_id = df.loc[df['tweet'] == tweets, 'tweet_id'].values[0]\n",
    "          \n",
    "          tokens = tokenizer.tokenize(tweets)\n",
    "          lemmatizer = WordNetLemmatizer()\n",
    "          lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "          tweets = ' '.join(lemmatized_tokens)\n",
    "          \n",
    "          df.loc[df['tweet_id']==tweet_id, 'tweet'] = tweets\n",
    "          \n",
    "     return df\n",
    "\n",
    "def removing_single_characters(df):\n",
    "     for tweets in df.loc[:,'tweet']:\n",
    "          \n",
    "          tokenizer = TweetTokenizer()\n",
    "          tweet_id = df.loc[df['tweet'] == tweets, 'tweet_id'].values[0]\n",
    "          \n",
    "          tokens = tokenizer.tokenize(tweets)\n",
    "          filtered_tokens = [word for word in tokens if len(word) > 1]\n",
    "          tweets = ' '.join(filtered_tokens)\n",
    "          \n",
    "          df.loc[df['tweet_id']==tweet_id, 'tweet'] = tweets\n",
    "          \n",
    "     return df\n",
    "\n",
    "def removing_tweets_less_5(df):\n",
    "     for tweets in df.loc[:, 'tweet']:\n",
    "          \n",
    "          tweet_id = df.loc[df['tweet'] == tweets, 'tweet_id'].values[0]\n",
    "          \n",
    "          if len(tweets.split()) < 5:\n",
    "               df.drop(df[df['tweet_id']==tweet_id].index, inplace=True)\n",
    "     return df\n",
    "\n",
    "def removing_duplicates(df):\n",
    "     df = df.drop_duplicates(subset=['tweet'])\n",
    "     return df\n",
    "\n",
    "def data_preprocessing(df):\n",
    "     \n",
    "     df = removing_links(df)\n",
    "     df = contractions_handling(df)\n",
    "     # df = adding_space_bw_words_punc(df)\n",
    "     df = removing_hashtags_mentions(df)\n",
    "     # df = removing_punctuations_emojis(df)\n",
    "     df = removing_numbers(df)\n",
    "     df = lemmatization(df)\n",
    "     df = removing_stopwords(df)\n",
    "     df = removing_single_characters(df)\n",
    "     df = removing_tweets_less_5(df)\n",
    "     df = removing_duplicates(df)\n",
    "     df.reset_index(drop=True, inplace=True)\n",
    "     \n",
    "     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mc/_plzx65559n4fxtnmj3tqk9w0000gp/T/ipykernel_23022/977751759.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.drop(df[df['tweet_id']==tweet_id].index, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18184, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>city</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>It bad thing say government medical spare oper...</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>netas family admitted government hospital Priv...</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Govandi one Hotspot well Respiratory Diseases ...</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Till medical negligence exist government hospi...</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Me doctor reading Also government hospital res...</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id                                              tweet    city  year\n",
       "0         0  It bad thing say government medical spare oper...  Mumbai  2022\n",
       "1         1  netas family admitted government hospital Priv...  Mumbai  2022\n",
       "2         2  Govandi one Hotspot well Respiratory Diseases ...  Mumbai  2022\n",
       "3         3  Till medical negligence exist government hospi...  Mumbai  2022\n",
       "4         4  Me doctor reading Also government hospital res...  Mumbai  2022"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_cleaned_df = data_preprocessing(tweets_no_dupl_df)\n",
    "print(tweets_cleaned_df.shape)\n",
    "tweets_cleaned_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'JMI', 'PO', 'WEA', 'NCI', 'AFMS', 'NZM', 'GHK', 'SAST', 'TY', 'AMU', 'BU', 'DAD', 'IGST', 'DMC', 'ACPD', 'RMO', 'MRC', 'BMJH', 'AT', 'CHS', 'IP', 'IN', 'SBC', 'GOK', 'KAR', 'SAW', 'GOVT', 'JPR', 'NAVY', 'EAST', 'TU', 'DFS', 'BS', 'SP', 'REG', 'CP', 'DIV', 'R.', 'MA', 'GALI', 'SAR', 'SGRH', 'HCG', 'NSUI', 'CGST', 'MY', 'MET', 'UPA', 'COP', 'WHAT', 'CMC', 'RSS', 'IS', 'PMS', 'USD', 'HBA', 'THR', 'PPH', 'CMRI', 'DIYA', 'WRT', 'UAPA', 'LPG', 'GPRA', 'IIMC', 'NOR', 'DOCS', 'GTR', 'UTs', 'CAR', 'NLUs', 'LWP', 'ADMK', 'SAHA', 'IUC', 'JAI', 'KDMC', 'AMHO', 'OT', 'CD', 'J.', 'USG', 'RAY', 'CK', 'CTP', 'GALL', 'QC', 'JNU', 'KPC', 'ESRD', 'LN', 'BDD', 'MAD', 'CAME', 'SK', 'RT', 'AGE', 'YAG', 'MRs', 'ELSE', 'SAIL', 'SAAS', 'IITM', 'DTCs', 'SION', 'KPIs', 'A.', 'AIMS', 'ZONE', 'MDL', 'THQ', 'NGEF', 'TTT', 'ORS', 'UAE', 'PPP', 'HOW', 'PAC', 'AMH', 'ET', 'CELL', 'BASE', 'BANK', 'AMID', 'RGI', 'HV', 'V.C.', 'MHRD', 'SER', 'MEN', 'B.S.', 'POOL', 'JCB', 'UPS', 'SUN', 'DA', 'EHS', 'CNG', 'PLZZ', 'TMA', 'TIA', 'DOO', 'MH', 'NCP', 'AES', 'SMHS', 'FDA', 'ALI', 'DP', 'GOES', 'DTC', '.MMS', '.RML', 'TYPE', 'GIVE', 'FORM', 'CGHS', 'UBER', 'ORM', 'FROM', 'DEMO', 'KMCH', 'K.K.', 'MLCs', 'LDF', 'NLU', 'SRM', 'MCS', 'CVTC', 'MMG', 'TRUE', 'DU', 'IPCC', 'RGIA', 'SCAM', 'PL', 'SEVA', 'RO', 'LHRC', 'GHMC', 'NWFP', 'PTI', 'GAON', 'YZQ', 'DMRC', 'PHT', 'LACS', 'CMG', 'FEES', 'JOIN', 'RUDE', 'FEE', 'BAAT', 'BG', 'DID', 'AGS', 'LAKH', 'RL', 'NCT', 'TKR', 'EXAM', 'CODE', 'JAIN', 'DNB', 'HAND', 'JAAN', 'JI', 'SUCK', 'WAY', 'CAA', 'GOV', 'DMs', 'PTC', 'MLAs', 'HABA', 'FACE', 'AISE', 'BAGH', 'GT', 'SHOP', 'SS', 'DRC', 'AR', 'K.', 'SVS', 'AB', 'HRR', 'KMH', 'AMO', 'HUM', 'CT', 'VPS', 'SIMS', 'MSR', 'YSR', 'THE', 'DEVI', 'CKS', 'MART', 'DCMS', 'JRs', 'ELIM', 'RAJA', 'CSR', 'PSU', 'ILBS', 'CMD', 'ANHA', 'SUCH', 'NRMU', 'DEDO', 'DNA', 'RWA', 'AAP', 'GOI', 'CCP', 'INC', 'COST', 'NPPA', 'CIT', 'DFO', 'RAMP', 'CTC', 'FEW', 'SEP', 'MGR', 'RAHE', '.ROT', 'TXNG', 'CCC', 'TN', 'HALF', 'SUVs', 'TPAs', 'UMID', 'EYE', 'BOOK', 'DIE', 'CFM', 'T.', 'KPD', 'CMI', 'ITI', 'DHA', 'DMA', 'SOAP', 'TC', 'NIT', 'RBC', 'GTB', 'SOME', 'OTs', 'VLC', 'LESS', 'RCC', 'DPCO', 'DCH', 'LK', 'ERP', 'BT', 'DDR', 'CPI', 'SOO', 'IHAS', 'CWC', 'CBSE', 'CIPA', 'BBC', 'SRK', 'RING', 'FIL', 'GOD.', 'ITR', 'MUST', 'FDI', 'V.', 'NRI', 'HAL', 'CBIC', 'MORE', 'DEEN', 'DOON', 'FIND', 'ICU', 'BHI', 'HMOs', 'ECHS', 'RDT', 'UID', 'HBS', 'TPA', 'RBI', 'SATH', 'PCK.', 'BRK', 'SN', 'RIMS', 'SDPI', 'GMCH', 'NAVI', 'INS', 'MHA', 'HQs', 'MRTP', 'MFIs', 'PROF', 'HRDA', 'CHCs', 'SPs', 'OPS', 'DM', 'APMC', 'KARA', 'LTMG', 'ADHD', 'ARE', 'EA', 'GOD', 'MHBS', 'KOTI', 'PCNL', 'HCA', 'WELL', 'AICC', 'LANE', 'OMR', 'LLRM', 'ISRO', 'IDH', 'OBN', 'BSA', 'HN', 'ART', 'JR', 'IR', 'SSB', 'OR', 'SYED', 'ARK', 'SEE', 'RDI', 'ISL', 'MAPA', 'CPAP', 'IBM', 'KNEE', 'OWN', 'DAYS', 'MRPs', 'AE', 'DHC', 'VMMC', 'KN', 'WAVE', 'TELL', 'AMA', 'NDA', 'STI', 'NFC', 'UC', 'ITBP', 'CBP', 'MCD', 'CLG', 'DOA', 'MSO', 'GOA', 'NDRF', 'ONCE', 'RBM', 'CESC', 'RRs', 'DHFL', 'HAVE', 'CCU', 'KEPT', 'USI', 'MRS', 'OUT', 'MICU', 'NYHA', 'EAT', 'MAA', 'DCHC', 'JAY', 'BODY', 'RNO', 'DOES', 'WTH', 'LACK', 'KB', 'LC', 'RJD', 'BAIL', 'NIRF', 'BOND', 'PCI', 'PVT', 'SPO', 'OCID', 'WERE', 'WBC', 'UTP', 'IO', 'GO', 'CRP', 'PNB', 'JSS', 'YOG', 'TEE', 'AIPC', 'TONE', 'AUG', 'LOW', 'USE', 'WISE', 'IMHO', 'JSPS', 'ICCU', 'DMS', 'DATE', 'NIV', 'UPSC', 'HUGE', 'TO', 'ML', 'ACB', 'A.P.', 'SIX', 'SAB', 'DEAF', 'RMOs', 'ATAL', 'TIME', 'COLD', 'FANS', 'CDS', 'FMG', 'DONE', 'UIP', 'NH', 'SNEH', 'F.', 'MAIN', '.PRV', 'BUCK', 'NDLS', 'BMC', 'BR', 'SOMA', 'TL', 'SNOW', 'UP.', 'IND.', 'HIV', 'GAS', 'ABLE', 'SBG', 'RUB', 'INTO', 'NOC.', 'SNCF', 'IFSC', 'HIGH', 'PNG', 'DOT', 'CAG', 'CPRs', 'HSG', 'MHUs', 'HDFC', 'ANI', 'COPD', 'GERD', 'LIVE', 'TAX', 'MIG', 'SEEK', 'PJ', 'GN', 'ALSO', 'SBI', 'AJMS', 'SAF', 'AIHM', 'DND', 'PVN', 'PCR', 'NEET', 'PMC', 'IBKL', 'APEX', 'SAID', 'SM', 'ASK', 'RAK', 'EE', 'GDP', 'DAP', 'BRD', 'LTGM', 'SSTR', 'DN', 'OM', 'CPIM', 'NVR', 'BAD', 'IGP', 'ANIL', 'ECHO', 'GOT', 'RV', 'MERF', 'HCT', 'ABVP', 'ARMY', 'WAIT', 'KGF', 'ED', 'MRP', 'MK', 'AA', 'ICMR', 'HAMM', 'MBBS', 'PHC', 'O.', 'YMCA', 'CSI', 'ANY', 'SEWA', 'WORD', 'BVM', 'IA', 'MANY', 'NGOs', 'IANS', 'WFA', 'NDMA', 'BQ', 'MCI', 'YS', 'CAN', 'CEA', 'BA', 'UPHC', 'THEN', 'PRO', 'OO', 'TOUS', 'UNDR', 'RG', 'SOON', 'EBER', 'OK', 'COMM', 'DGO', 'TR', 'SDRF', 'EACH', 'SCIL', 'SL', 'YO', 'IABP', 'TET', 'US.', 'PLUS', 'M.', 'HYD', 'CS', 'SHOW', 'ZP', 'RMLH', 'KAUR', 'STCF', 'RBS', 'NDTV', 'SLG', 'IVs', 'VD', 'TJ', 'FAIL', 'EMIs', 'LS', 'EC', 'BEST', 'GATE', 'ACT', 'KUCH', 'HRS', 'GUYS', 'DOWN', 'BYJD', 'DEL', 'RCA', 'EV', 'GST', 'URI', 'PHD', 'STAY', 'LOT', 'DENS', 'SJM', 'TIMS', 'STAR', 'IC', 'SNS', 'SURE', 'KS', 'DJ', 'JNTU', 'MHC', 'FOOT', 'DOG', 'LT', 'III', 'ANTI', 'AVM', 'PVC', 'OGH', 'ITC', 'AED', 'SC', 'CARs', 'SO', 'NAME', 'MEDs', 'IIM', 'DAMA', 'NBE', 'KA', 'NMCH', 'MO', 'SLOW', 'HOTA', 'RS', 'MGS', 'MCs', 'NRS', 'SANE', 'DMK', 'XXX', 'ECG', 'NEW', 'BABI', 'IV', 'OTP', 'OYO', 'HK', 'CJ', 'CRPF', 'APNA', 'PI', 'DCP', 'MNP', 'DDA', 'CON', 'IPD', 'TILL', 'TRP', 'HOUR', 'GOI.', 'NCR', 'GOUD', 'SSI', 'MBA', 'CICU', 'SAAB', 'GVG', 'TAKE', 'BAN', 'AID', 'LIFE', 'LATE', 'MIRA', 'ACH', 'CIB', 'RP', 'GIT', 'BLUE', 'FIRE', 'HELL', 'TTD', 'HE', 'PCs', 'IPC', 'HA', 'COWS', 'RCT', 'WORK', 'SON', 'BILL', 'BOG', 'NAKA', 'BYL', 'ME', 'IND', 'TFM', 'TOLL', 'SARS', 'UG', 'HR', 'CMR', 'RAO', 'OPC', 'DDCA', 'UTC', 'APP', 'AIR', 'LAST', 'VPO', 'IVF', 'SARI', 'PGs', 'LAND', 'AUNT', 'HCMS', 'TN.', 'BCV', 'APPA', 'THAN', 'WFC', 'MOH', 'YES', 'ISIC', 'MG', 'IDBI', 'SKMC', 'A.R.', 'RTR', 'KYA', 'IMAX', 'R.P.', 'TISS', 'AQI', 'DIST', 'LOST', 'RTC', 'POTA', 'CTS', 'MT', 'CNC', 'WC', 'AGO', 'KAML', 'RAF', 'BMT', 'LOOK', 'RN', 'TMC', 'ACMS', 'MOST', 'AMCs', 'DAY', 'VAMC', 'ANM', 'DGD', 'TODI', 'VUJ', 'RAT', 'POST', 'UP', 'IDA', 'BJ', 'ILL', 'MMA', 'CBC', 'FREE', 'OC', 'GOLD', 'VIII', 'LOL', 'JOB', 'PAR', 'HIS', 'BRTS', 'GDA', 'MCDs', 'CVC', 'WUMS', 'EMI', 'PF', 'TEAM', 'PLEA', 'HSR', 'IIT', 'FMGE', 'DD', 'SUV.', 'PPES', 'END', 'KV', 'CBI', 'BSES', 'MARD', 'HIM', 'DIG', 'GGN', 'GHA', 'COME', 'XDR', 'ISBT', 'CARD', 'G.B.', 'JN', 'MMC', 'PJR', 'KIND', 'THEM', 'DCW', 'H.F.', 'HCQS', 'FS', 'B.R.', 'BBM', 'OPEN', 'RDC', 'LADY', 'TB.', 'NPA', 'ISPR', 'KPHB', 'DOC', 'BLK', 'YANA', 'LOSE', 'PIL', 'WISH', 'OPP', 'BGS', 'TATA', 'HOME', 'AML', 'ITPL', 'PSM', 'BORN', 'CMS', 'WEST', 'NL', 'MDR', 'SEC', 'NSG', 'GYM', 'II', '.P.', 'NLEM', 'BGH', 'RIP', 'MLD', 'TQ', 'TH', 'EPS', 'PCF', 'LV', 'IMV', 'MCCD', 'VSD', '.SDM', 'EID', 'BABA', 'GET', 'BAMS', 'NMC', 'GB', 'MEIN', 'ELSO', 'RCMG', 'KG', 'CPD', 'DRMs', 'KRK', 'M.R.', 'OMNI', 'NASA', 'HWH', 'SAVE', 'ITS', 'FMCG', 'LD', 'MSC', 'MUCH', 'OMG', 'CEO', 'NOK', 'CM.', 'CGI', 'ER', 'BCMC', 'ROKO', 'SV', 'S.', 'DX', 'WALA', 'UCs', 'GODS', 'BLS', 'ILD', 'NCB', 'AND', 'VN', 'MAX', 'THEY', 'FMB', 'VIN', 'SMRC', 'KHAN', 'LAB', 'CFL', 'GIMS', 'MLA', 'GONE', 'BPL', 'BHU', 'PFA', '.KTR', 'IRDA', 'NAIR', 'CLOT', 'ABC', 'CTVS', 'GBR', 'WITH', 'DAV', 'CMO', 'B.K.', 'CEOs', 'HOGA', 'MW', 'HH', 'MMR', 'LCHF', 'HCPs', 'DBR', 'ESMA', 'ASHA', 'M.G.', 'HOSP', 'YOU', 'GRs', 'MR', 'ISI', 'RAGA', 'JLN', 'SDP', 'ESIC', 'HS', 'CHD', 'BK', 'DG', 'NGS', 'JNV', 'THIS', 'GAO', 'SSP', 'KYC', 'SGPI', 'EMR', 'BABU', 'UGs', 'LICK', 'DGP', 'OMC', 'NBC', 'HI', 'ABP', 'SR', 'NKS', 'KPME', 'MCQ', 'UK.', 'NP', 'PICU', 'MAN.', 'RAM', 'YOUR', 'BEDS', 'DRDO', 'GOOD', 'MAMC', 'CQC', 'PHO', 'CVO', 'BUMS', 'WHO', 'AMC', 'FD', 'IPS', 'SKMS', 'NSP', 'RUNS', 'NHPS', 'CPR', 'BB', 'MDMS', 'SSKM', 'MHAD', 'MCWC', 'VHS', 'DUO', 'MNS', 'DULY', 'RUN', 'TEST', 'NHs', 'NDMC', 'MPs', 'LTD', 'MASK', 'DO', 'FLU', 'CL', 'HBT', 'BED', 'EMO', 'PC', 'WILL', 'MCGM', 'GI', 'EMRs', 'LBS', 'PV', 'IQ', 'AIDS', 'MVA', 'MRCS', 'SRs', 'SSR', 'SICU', 'DL', 'NAA', 'CARE', 'FYA', 'BOW', 'WOW', 'LIG', 'ORR', 'ECP', 'NEWS', 'BUDH', 'ROFL', 'IAM', 'RISK', 'HBD', 'FB', 'NON', 'NEXT', 'DK', 'NCLT', 'DMCH', 'LIFT', 'HDU', 'SCR', 'HAI', 'PSO', 'LVL', 'ONLY', 'HFNC', 'CABG', 'FYS', 'RPC', 'KUB', 'VENU', 'HER', 'INX', 'TBA', 'FYCK', 'DRUG', 'DR.', 'RC', 'SET', 'ETA', 'IMR', 'KR', 'AMG', 'PAST', 'NSA', 'LIYE', 'PARA', 'PAY', 'GRP', 'EWS', 'SFI', 'WAQF', 'NTB', 'KMF', 'EAM', 'DLI', 'FAQs', 'MP', 'NIMS', 'KO', 'JP', 'HIND', 'EARN', 'CMH', 'TSUA', 'TNMC', 'WARD', 'DIS', 'BKC', 'NUH', 'RATE', 'HMS', 'WIFI', 'CMOs', 'URL', 'GOTG', 'HSCB', 'NOT', 'AGAR', 'VIP', 'NEO', 'RIS', '.SP.', 'PMs', 'WHEN', 'AAP.', 'CSF', 'US', 'NAV', 'IMCT', 'TMU', 'IST', 'DSSC', 'NA', 'REAL', 'AUTO', 'LSCS', 'ROAD', 'MAKE', 'DARA', 'SSC', 'PEAD', 'KDAH', 'UMAR', 'ASA', 'ADRM', 'SOPs', 'RWAs', 'SB', 'FIX', 'JUDA', 'SRA', 'SHSS', 'LL', 'LMB', 'JUST', 'HCQ', 'AM', 'HARD', 'NIBM', 'SUB', 'SRS', 'CAT', 'GP', 'NIA', 'LIKE', 'TOO', 'UK', 'PAF', 'CVD', 'XI', 'FOOD', 'AV', 'BBMP', 'YRS', 'BUT', 'SA', 'ADM', 'BDS', 'ESIS', 'WTF', 'RML', 'PCC', 'NICU', 'CUG', 'NBCC', 'HAH', 'NHRC', 'MM', 'KCR', 'NGO', 'PR', 'JRH', 'HSIS', 'DMX', 'DIN', 'ASI', 'KJ', 'HOPE', 'TOOS', 'TG', 'BMTC', 'LOUD', 'RAVI', 'DSA', 'N.', 'HODs', 'CJI', 'MBMC', 'SHOE', 'SOLD', 'NABH', 'GZB', 'NEED', 'PRK', 'CME', 'BMS', 'FN', 'ICF', 'NITI', 'BGM', 'KEEP', 'PATA', 'HRCT', 'SHE', 'LOSS', 'BTM', 'GMC', 'TAB', 'DEAD', 'REST', '.BMC', 'FIMS', 'RGGH', 'IVR', '.SEE', 'LO', 'TMH', 'NSCI', 'HOD', 'PAN', 'BCCI', 'GM', 'PWD', 'FIRs', 'WIN', 'LAWS', 'CNBC', 'WB.', 'BAI', 'TWO', 'JEE', 'ADGP', 'MD', 'GRID', 'OF', 'UCMS', 'AI', 'SPG', 'ISHU', 'CVTV', 'EYES', 'ASIT', 'POVs', 'JAM', 'B.', 'EMS', 'CAs', 'GK', 'ECO', 'MPDA', 'SHO', 'PCRs', 'SI', 'JOBS', 'SIMs', 'ETC', 'STOP', 'OVER', 'FOB', 'MRCC', 'SAYS', 'CSC', 'NDIA', 'PM.', 'PICK', 'SNR', 'L.', 'CV', 'SBS', 'GQ', 'POOR', 'UN', 'PNS', 'WO', 'PRE', 'BGML', 'HCW', 'HT', 'CBD', 'BMI', 'SICK', 'NRWA', 'MCP', 'SPOT', 'TTE', 'CISF', 'H.D.', 'LEG', 'VIPs', 'MERA', 'AUR', 'CALL', 'AMRI', 'KNN', 'SWG', 'POLL', 'ON', 'MIDC', 'LHMC', '.TX', 'JMJ', 'CUP', 'VC', 'BUZZ', 'OSA', 'AAJ', 'QR', 'RSA', 'MISS', 'FCI', 'MNCs', 'BLR', 'NOW', 'BSP', 'TURN', 'HP', 'PGIs', 'SRV', 'BIG', 'TGA', 'SMGS', 'AVF', 'FULL', 'JMML', 'PACE', 'LLM', 'AMDT', 'DAK', 'IF', 'HM', 'BY', 'RULE', 'GE', 'BW', 'BD', 'CCS', 'GYMS', 'IES', 'MLC', 'GRE', 'HCU', 'MPP', 'HRD', 'ECIL', 'TS', 'HB', 'VLCC', 'TDP', 'SCTF', 'BRO', 'PHON', 'TSRs', 'HDF', '.LAB', 'MRNA', 'SAD', 'SEEN', 'STPs', 'BIBI', 'TV.', 'PRV', 'PLS', 'RMP', 'JVLR', 'TPR', 'WEAR', 'MCF', 'UPI', 'WA', 'COZD', 'HON', 'KIDS', 'KICK', 'PRAY', 'CA', 'TUM', 'MHAW', 'CCTV', 'PPEs', 'TOS', 'ERCP', 'GURU', 'ZOI', 'ID', 'SMS', 'BSNL', 'DIPP', 'POK', 'WB', 'ERT', 'YOGI', 'FWB', 'BABY', 'KMs', 'SDMC', 'SRCC', 'DLZB', 'HMWS', 'IAF', 'ABCD', 'SUM', 'USED', 'MIN', 'OGH.', 'FEAR', 'MAAR', 'NOs', 'MODI', 'DMHO', 'MLY', 'MC', 'JJ', 'BILE', 'NGR', 'PUT', 'ZERO', 'INR', 'FARM', 'ESI', 'ST', 'RE', 'CAPF', 'SVM', 'GOLF', 'TIL', 'ISHA', 'PCOS', 'HCWs', 'HDI', 'LB', 'BMX', 'DHS', 'PPE', 'MTPs', 'CAB', 'EDMC', 'PAAN', 'CCB', 'PACS', 'KIMS', 'DTH', 'ENT', 'GOON', 'PCOD', 'H.', 'MONY', 'BV', 'NYC', 'MENT', 'NMMC', 'MSME', 'BOLD', 'PLA', 'MIND', 'VS', 'IMS', 'PMCH', 'PARK', 'AAPs', 'MOs', 'PS', 'STN', 'DS', 'NEAR', 'P.C.', 'PAIN', 'IT', 'IIMs', 'SOP', 'AIG', 'CN', 'QRT', 'GAIN', 'AMCU', 'EB', 'BJYM', 'MMCT', 'VERY', 'CE', 'GBN', 'BUSY', 'VK', 'LELA', 'PILs', 'NHS', 'UHID', 'YGB', 'DY', 'AS', 'MED', 'HOO', 'HAIN', 'LLR', 'CNN', 'FPMS', 'NO', 'UCAT', 'IITD', 'MTR', 'MSH', 'LNJP', 'MHEU', 'ROCK', 'BAL', 'HAS', 'MAN', 'IMG', 'SVBP', 'GNCT', 'CONG', 'JIO', 'DC', 'ADC', 'FOC', 'OBG', 'WAS', 'COM', 'DH', 'SAFE', 'G.', 'WEB', 'MMS', 'SAI', 'TOF', 'TRAP', 'GG', 'RTA', 'BSF', 'RAJ', 'IG', 'CVs', 'NIL', 'AJ', 'TOP', 'MHEs', 'KE', 'CMs', 'PVR', 'MADE', 'BALA', 'MRD', 'BL', 'CHOR', 'CW', 'NHA', 'USA', 'KAHI', 'IPs', 'GGP', 'SJH', 'NATA', 'PIN', 'CIMS', 'OUR', 'KBR', 'G.K.', 'HPMS', 'PH', 'SRC', 'GIRL', 'IHF', 'DPS', 'IPL', 'PG', 'KAL', 'MS', 'DON', '.GOI', 'MGM', 'IYC', 'CHC', 'WLO', 'ITU', 'ICUS', 'SRN', 'OPD.', 'PASS', 'IDY', 'MTNL', 'MAR', 'RK', 'STF', 'RUBY', 'PHCs', 'THAT', 'RNC', 'ABN', 'IHRO', 'RAJU', 'STD', 'SRCM', 'OFF', 'DGCA', 'CURI', 'IAS', 'FIR', 'GUY', 'NOV', 'FALL', 'HELP', 'EVEN', 'ETP', 'ZPHS', 'OKAY', 'PDS', 'DMB', 'LNH', 'MBAs', 'KMC', 'SCB', 'FM', 'HEAL', 'UT', 'CITY', 'W.B.', 'BP', 'AREA', 'YSK', 'CBN', 'AC', 'HQ', 'FAKE', '.RTE', 'MUV', 'BMO', 'FAST', 'VG', 'OSD', 'CPEC', 'AEFI', 'SRMC', 'NSSH', 'MRI', 'RM', 'FACT', 'DLPC', 'KAB', 'TUMB', 'LYF', 'GIAN', 'TVs', 'JK', 'CPM', 'NGT', 'GSVM', 'EPC', 'FILE', 'KC', 'CPC', 'PT', 'NOD', 'MIOT', 'KI', 'GSC', 'JJM', 'SE', 'MOU', 'NABL', 'ES', 'UNIT', 'VDS', 'KUD', 'PRN', 'RBTB', 'FUCK', 'NOTE', 'LAD', 'ICUs', 'BJP', 'ATS', 'VVIP', 'RPG', 'UTR', 'GS', 'GAVE', 'CSMT', 'HOBE', 'SCUP', 'STP', 'VVMC', 'GKP', 'MHU', 'SAY', 'PANT', 'CDC', 'JBC', 'KM', 'HG', 'RTI', 'MB', 'RDA', 'PUNE', 'SDN', 'HAHC', 'HW', 'MSM', 'TRA', 'FUND', 'SAME', 'KOL', 'DOSE', 'HANB', 'ROOM', 'AAM', 'PSRI', 'NHPM', 'RIDE', 'MAT', 'CM', 'TREE', 'SVMC', 'PFI', 'ECMO', 'BARC', 'S.K.', 'AP', 'AN', 'WALK', 'SRF', 'FSM', 'AKN', 'NRC', 'DNA.', 'WE', 'WAKE', 'WFH', 'PNR', 'NOC', 'SVP', 'ADR', 'MARG', 'PD', 'EXTN', 'CO', 'WANT', 'PM', 'IHD', 'MPM', 'AK', 'MRN', 'JOKE', 'SGST', 'HALL', 'CTI', 'BNV', 'TOD', 'KTR', 'Y.S.', 'PHR', 'LOTS', 'TOI', 'MKCG', 'ONE', 'DGHS', 'CRT', 'ACP', 'VE', 'OPD', 'PTA', 'KNOW', 'CC', 'CX', 'KEM', 'PGI', 'UTI', 'BEEN', 'DF', 'SG', 'PCT', 'CMRF', 'AITC', 'BIL', 'KHT', 'BIEC', 'TRY', 'RJS', 'HBTH', 'DDU', 'AFI', 'KBK', 'LKO', 'ACWA', 'MAY', 'LOOT', 'ZIA', 'GH', 'MILK', 'CCI', 'COLS', 'RIF', '.UP', 'BBSR', 'MTB', 'BPO', 'LONG', 'BIN', '.BJP', 'GA', 'DGPs', 'CST', 'MIL', 'HRBR', 'LLP', 'IPO', 'CG', 'DLF', 'MKS', 'ND', 'OLD', 'HCL', 'KP', 'LG', 'TT', 'INNR', 'RPF', 'WIFA', 'SIR', 'LMA', 'LMO', 'BAPS', 'KITS', 'GGS', 'YCP', 'URDA', 'CUM', 'GRR', 'RUYA', 'KL', 'DIDI', 'LOAD', 'NLC', 'NAHI', 'RR', 'NRIs', 'PMO', 'MHDG', 'HL', 'PMDD', 'CLAD', 'TX', 'QRG', 'CSR.', 'DOTS', 'EJS', 'BHK', 'WIFE', 'NTR', 'TB', 'NE', 'PK', 'MSW', 'FSL', 'N.J.', 'AGED', 'UHC', 'CR', 'MPCT', 'BDA', 'NHM', 'ACI', 'DUTY', 'GPs', 'ATM', 'IGM', 'BSC', 'IMP', 'GFs', 'SDM', 'PLZ', 'TV', 'BEG', 'EVER', 'DUE', 'AMD', 'NKDA', 'FOR', 'BNS', 'GSM', 'PSA', 'LIST', 'MKG', 'WASH', 'HC', 'OLA', 'ROI', 'A.K.', 'JIMS', 'DR', 'IL', 'MPPS', 'VOTE', 'TSH', 'ALL', 'RD', 'OPDs', 'YY', 'HIP', 'SD', 'GD', 'HEAR', 'DUST', 'IPHS', 'NM', 'ZTCC', 'WHY', 'SIGN', 'YET', 'DCM', 'RLM', 'FYI', 'GPD', 'CASE', 'BE', 'BACK', 'SKB', 'OP', 'SIDE', 'UV', 'PER', 'MOS', 'HAD', 'VVPI', 'GBS', 'ACLS', 'PB', 'VDRL', 'KGH', 'HD', 'RAIL', 'BUS', 'ISIS', 'UNKE', 'INU', 'VB', 'TAG', 'VP', 'SEZ', 'PCPO', 'KIT', 'TRS', 'TNN', 'WCD', 'NIH', 'DEAR', 'NR', 'AFP', 'NOPE', 'BPT'}\n"
     ]
    }
   ],
   "source": [
    "def get_abbreviations(df):\n",
    "    count = 0\n",
    "    abb = list()\n",
    "    for tweets in df.loc[:, 'tweet']:\n",
    "        count+=1\n",
    "        # finding abbreviations in the tweets\n",
    "        x = re.findall(r\"\\b[A-Z\\.]{2,}s?\\b\", tweets)\n",
    "        if len(x) > 0:\n",
    "            # print(x)\n",
    "            for i in x:\n",
    "                if len(i)<5:\n",
    "                    abb.append(i)\n",
    "    print(set(abb))\n",
    "\n",
    "get_abbreviations(tweets_cleaned_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hinglish_to_english(df, lang):\n",
    "    translated_val = list()\n",
    "    count = 0\n",
    "    # translating the text to english using GoogleTranslator API\n",
    "    for x in df['tweet']:\n",
    "        count += 1\n",
    "        try:\n",
    "            # GoogleTranslator API has a limit of 5000 characters per request, so splitting the text into chunks of 5000 characters and then translating it\n",
    "            if len(str(x))<5000:\n",
    "                \n",
    "                translation = GoogleTranslator(source=lang, target='en').translate(x)\n",
    "                translated_val.append(translation)\n",
    "            \n",
    "            elif len(str(x))>5000 and len(str(x))<10000:\n",
    "                \n",
    "                split_x = [x[i:i+4999] for i in range(0, len(x), 4999)]\n",
    "                translation_1 = GoogleTranslator(source=lang, target='en').translate(split_x[0])\n",
    "                translation_2 = GoogleTranslator(source=lang, target='en').translate(split_x[1])\n",
    "                translation = translation_1 + translation_2\n",
    "                translated_val.append(translation)\n",
    "                \n",
    "        except Exception as e:\n",
    "            # if the text is not in the language provided, then it will return a nan value or text length is more than 15000 characters\n",
    "            translated_val.append(np.nan)\n",
    "        if count%1000==0:\n",
    "            print(count)\n",
    "    \n",
    "    # replacing the original text with the translated text\n",
    "    df['tweet_translated'] = translated_val\n",
    "    \n",
    "    # returning the updated dataframe\n",
    "    return df\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "translated_df = hinglish_to_english(tweets_cleaned_df, 'hi')\n",
    "translated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.to_csv(translated_df, '/Users/nitanshjain/Documents/Projects/Twitter_Data_Analysis/v2/data/tweets_translated_v2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to create a document-term matrix using CountVectorizer, and exclude common English stop words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(stop_words='english', min_df=10, ngram_range=(1,1))\n",
    "data_clean_cv = cv.fit_transform(translated_df.tweet)\n",
    "# data_clean_cv.toarray()\n",
    "data_clean_dtm = pd.DataFrame(data_clean_cv.toarray(), columns=cv.get_feature_names_out())\n",
    "data_clean_dtm.index = translated_df.index\n",
    "data_clean_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating csv of cleaned dataset\n",
    "pd.DataFrame.to_csv(tweets_cleaned_df, '/Users/nitanshjain/Documents/Projects/Twitter_Data_Analysis/v2/data/tweets_cleaned_with_stopwords_v2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Twitter_Data_Analysis-wZ120kVj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
