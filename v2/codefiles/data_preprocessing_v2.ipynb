{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/nitanshjain/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/nitanshjain/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/nitanshjain/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "import fnmatch\n",
    "import string\n",
    "from urllib.parse import urlparse\n",
    "import contractions\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('words')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, TweetTokenizer\n",
    "from nltk.corpus import words, stopwords\n",
    "from nltk.metrics.distance import jaccard_distance, edit_distance\n",
    "from nltk.util import ngrams\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from deep_translator import GoogleTranslator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26015, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>date_created</th>\n",
       "      <th>tweet</th>\n",
       "      <th>city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2022-12-12 16:13:45+00:00</td>\n",
       "      <td>@esichq @byadavbjp @Rameswar_Teli @mygovindia ...</td>\n",
       "      <td>Mumbai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2022-12-10 06:30:56+00:00</td>\n",
       "      <td>@PotholeWarriors @CMOMaharashtra @mieknathshin...</td>\n",
       "      <td>Mumbai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2022-11-23 13:09:18+00:00</td>\n",
       "      <td>@Iam_Ayushmann Govandi is one of the Hotspot o...</td>\n",
       "      <td>Mumbai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2022-10-27 15:58:11+00:00</td>\n",
       "      <td>Till when medical negligence will exist in gov...</td>\n",
       "      <td>Mumbai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2022-07-28 03:03:15+00:00</td>\n",
       "      <td>Me being a doctor reading this\\nAlso governmen...</td>\n",
       "      <td>Mumbai</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0               date_created  \\\n",
       "0           0  2022-12-12 16:13:45+00:00   \n",
       "1           1  2022-12-10 06:30:56+00:00   \n",
       "2           2  2022-11-23 13:09:18+00:00   \n",
       "3           3  2022-10-27 15:58:11+00:00   \n",
       "4           4  2022-07-28 03:03:15+00:00   \n",
       "\n",
       "                                               tweet    city  \n",
       "0  @esichq @byadavbjp @Rameswar_Teli @mygovindia ...  Mumbai  \n",
       "1  @PotholeWarriors @CMOMaharashtra @mieknathshin...  Mumbai  \n",
       "2  @Iam_Ayushmann Govandi is one of the Hotspot o...  Mumbai  \n",
       "3  Till when medical negligence will exist in gov...  Mumbai  \n",
       "4  Me being a doctor reading this\\nAlso governmen...  Mumbai  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df = pd.read_csv('/Users/nitanshjain/Documents/Projects/Twitter_Data_Analysis/v2/data/tweets_v2.csv')\n",
    "print(tweets_df.shape)\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet_id         int64\n",
      "date_created    object\n",
      "tweet           object\n",
      "city            object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "tweets_df.rename(columns = {'Unnamed: 0':'tweet_id'}, inplace=True)\n",
    "print(tweets_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>city</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@esichq @byadavbjp @Rameswar_Teli @mygovindia ...</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>@PotholeWarriors @CMOMaharashtra @mieknathshin...</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>@Iam_Ayushmann Govandi is one of the Hotspot o...</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Till when medical negligence will exist in gov...</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Me being a doctor reading this\\nAlso governmen...</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id                                              tweet    city  year\n",
       "0         0  @esichq @byadavbjp @Rameswar_Teli @mygovindia ...  Mumbai  2022\n",
       "1         1  @PotholeWarriors @CMOMaharashtra @mieknathshin...  Mumbai  2022\n",
       "2         2  @Iam_Ayushmann Govandi is one of the Hotspot o...  Mumbai  2022\n",
       "3         3  Till when medical negligence will exist in gov...  Mumbai  2022\n",
       "4         4  Me being a doctor reading this\\nAlso governmen...  Mumbai  2022"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#selecting only year from date_created column\n",
    "tweets_df['date_created'] = pd.to_datetime(tweets_df['date_created'])\n",
    "tweets_df['year'] = tweets_df['date_created'].dt.year\n",
    "tweets_df.drop(['date_created'], axis=1, inplace=True)\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delhi        10005\n",
      "Mumbai        6715\n",
      "Hyderabad     3597\n",
      "Bangalore     3229\n",
      "Kolkata       1413\n",
      "Chennai       1056\n",
      "Name: city, dtype: int64\n",
      "2020    8963\n",
      "2021    7255\n",
      "2019    3885\n",
      "2022    3214\n",
      "2018    2698\n",
      "Name: year, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(tweets_df.city.value_counts())\n",
    "print(tweets_df.year.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset before removal of duplicates is (26015, 4)\n",
      "Shape of dataset after removal of duplicates is (19350, 4)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of dataset before removal of duplicates is {}'.format(tweets_df.shape))\n",
    "tweets_no_dupl_df = tweets_df.drop_duplicates(subset=['tweet'])\n",
    "print('Shape of dataset after removal of duplicates is {}'.format(tweets_no_dupl_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delhi        7379\n",
      "Mumbai       4963\n",
      "Hyderabad    2614\n",
      "Bangalore    2471\n",
      "Kolkata      1077\n",
      "Chennai       846\n",
      "Name: city, dtype: int64\n",
      "2020    6517\n",
      "2021    5515\n",
      "2019    2767\n",
      "2022    2454\n",
      "2018    2097\n",
      "Name: year, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(tweets_no_dupl_df.city.value_counts())\n",
    "print(tweets_no_dupl_df.year.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def removing_links(df):\n",
    "     for tweets in df.loc[:,'tweet']:\n",
    "          \n",
    "          tokenizer = TweetTokenizer()\n",
    "          tweet_id = df.loc[df['tweet'] == tweets, 'tweet_id'].values[0]\n",
    "        \n",
    "          tokens = tokenizer.tokenize(tweets)\n",
    "          x = [word for word in tokens if not urlparse(word).scheme]\n",
    "          tweets = ' '.join(x)\n",
    "          \n",
    "          df.loc[df['tweet_id']==tweet_id, 'tweet'] = tweets\n",
    "          \n",
    "     return df\n",
    "\n",
    "def contractions_handling(df):\n",
    "     for tweets in df.loc[:,'tweet']:\n",
    "          \n",
    "          tweet_id = df.loc[df['tweet'] == tweets, 'tweet_id'].values[0]\n",
    "          tweets = contractions.fix(tweets)\n",
    "          \n",
    "          df.loc[df['tweet_id']==tweet_id, 'tweet'] = tweets\n",
    "          \n",
    "     return df\n",
    "\n",
    "def adding_space_bw_words_punc(df):\n",
    "     for tweets in df.loc[:, 'tweet']:\n",
    "          \n",
    "          tokenizer = TweetTokenizer()\n",
    "          tweet_id = df.loc[df['tweet'] == tweets, 'tweet_id'].values[0]\n",
    "          tweets = tweets.replace(',', ' , ').replace('.', ' . ').replace('?', ' ? ').replace('!', ' ! ')\n",
    "          \n",
    "          df.loc[df['tweet_id']==tweet_id, 'tweet'] = tweets\n",
    "     \n",
    "     return df\n",
    "\n",
    "def removing_hashtags_mentions(df):\n",
    "     for tweets in df.loc[:, 'tweet']:\n",
    "          \n",
    "          tokenizer = TweetTokenizer()\n",
    "          tweet_id = df.loc[df['tweet'] == tweets, 'tweet_id'].values[0]\n",
    "          \n",
    "          tokens = tokenizer.tokenize(tweets)\n",
    "          tokens = [word for word in tokens if word[0] not in ('#', '@')]\n",
    "          tokens = [word for word in tokens if word[0] not in ('▪')]\n",
    "          tweets = ' '.join(tokens)\n",
    "          \n",
    "          df.loc[df['tweet_id']==tweet_id, 'tweet'] = tweets\n",
    "     \n",
    "     return df\n",
    "          \n",
    "def removing_punctuations_emojis(df):\n",
    "     for tweets in df.loc[:, 'tweet']:\n",
    "          \n",
    "          tokenizer = TweetTokenizer()\n",
    "          tweet_id = df.loc[df['tweet'] == tweets, 'tweet_id'].values[0]\n",
    "          \n",
    "          tweets = tweets.translate(str.maketrans('', '', string.punctuation)) # removes punctuations\n",
    "          tweets = re.sub(r'[^\\x00-\\x7F]+', ' ', tweets) # removes emojis\n",
    "          tweets = tweets.lower() # converts text to lower case\n",
    "          \n",
    "          df.loc[df['tweet_id']==tweet_id, 'tweet'] = tweets\n",
    "     \n",
    "     return df\n",
    "\n",
    "def removing_numbers(df):\n",
    "     for tweets in df.loc[:, 'tweet']:\n",
    "          \n",
    "          tokenizer = TweetTokenizer()\n",
    "          tweet_id = df.loc[df['tweet'] == tweets, 'tweet_id'].values[0]\n",
    "          \n",
    "          #remove numbers\n",
    "          tweets = re.sub(r'\\d+', '', tweets)\n",
    "          tweets = re.sub(' +', ' ', tweets)\n",
    "          \n",
    "          df.loc[df['tweet_id']==tweet_id, 'tweet'] = tweets\n",
    "     \n",
    "     return df\n",
    "\n",
    "def removing_stopwords(df):\n",
    "     for tweets in df.loc[:,'tweet']:\n",
    "          \n",
    "          tokenizer = TweetTokenizer()\n",
    "          tweet_id = df.loc[df['tweet'] == tweets, 'tweet_id'].values[0]\n",
    "          \n",
    "          tokens = tokenizer.tokenize(tweets)\n",
    "          filtered_tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "          tweets = ' '.join(filtered_tokens)\n",
    "          \n",
    "          df.loc[df['tweet_id']==tweet_id, 'tweet'] = tweets\n",
    "          \n",
    "     return df\n",
    "\n",
    "def lemmatization(df):\n",
    "     for tweets in df.loc[:,'tweet']:\n",
    "          \n",
    "          tokenizer = TweetTokenizer()\n",
    "          tweet_id = df.loc[df['tweet'] == tweets, 'tweet_id'].values[0]\n",
    "          \n",
    "          tokens = tokenizer.tokenize(tweets)\n",
    "          lemmatizer = WordNetLemmatizer()\n",
    "          lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "          tweets = ' '.join(lemmatized_tokens)\n",
    "          \n",
    "          df.loc[df['tweet_id']==tweet_id, 'tweet'] = tweets\n",
    "          \n",
    "     return df\n",
    "\n",
    "def removing_single_characters(df):\n",
    "     for tweets in df.loc[:,'tweet']:\n",
    "          \n",
    "          tokenizer = TweetTokenizer()\n",
    "          tweet_id = df.loc[df['tweet'] == tweets, 'tweet_id'].values[0]\n",
    "          \n",
    "          tokens = tokenizer.tokenize(tweets)\n",
    "          filtered_tokens = [word for word in tokens if len(word) > 1]\n",
    "          tweets = ' '.join(filtered_tokens)\n",
    "          \n",
    "          df.loc[df['tweet_id']==tweet_id, 'tweet'] = tweets\n",
    "          \n",
    "     return df\n",
    "\n",
    "def removing_tweets_less_5(df):\n",
    "     for tweets in df.loc[:, 'tweet']:\n",
    "          \n",
    "          tweet_id = df.loc[df['tweet'] == tweets, 'tweet_id'].values[0]\n",
    "          \n",
    "          if len(tweets.split()) < 5:\n",
    "               df.drop(df[df['tweet_id']==tweet_id].index, inplace=True)\n",
    "     return df\n",
    "\n",
    "def removing_duplicates(df):\n",
    "     df = df.drop_duplicates(subset=['tweet'])\n",
    "     return df\n",
    "\n",
    "def data_preprocessing(df):\n",
    "     \n",
    "     df = removing_links(df)\n",
    "     df = contractions_handling(df)\n",
    "     df = adding_space_bw_words_punc(df)\n",
    "     df = removing_hashtags_mentions(df)\n",
    "     df = removing_punctuations_emojis(df)\n",
    "     df = removing_numbers(df)\n",
    "     df = lemmatization(df)\n",
    "     df = removing_stopwords(df)\n",
    "     df = removing_single_characters(df)\n",
    "     df = removing_tweets_less_5(df)\n",
    "     df = removing_duplicates(df)\n",
    "     df.reset_index(drop=True, inplace=True)\n",
    "     \n",
    "     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mc/_plzx65559n4fxtnmj3tqk9w0000gp/T/ipykernel_16939/2309682723.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.drop(df[df['tweet_id']==tweet_id].index, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18044, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>city</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>bad thing say government medical spare operati...</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>netas family admitted government hospital priv...</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>govandi one hotspot well respiratory disease c...</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>till medical negligence exist government hospi...</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>doctor reading also government hospital resident</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id                                              tweet    city  year\n",
       "0         0  bad thing say government medical spare operati...  Mumbai  2022\n",
       "1         1  netas family admitted government hospital priv...  Mumbai  2022\n",
       "2         2  govandi one hotspot well respiratory disease c...  Mumbai  2022\n",
       "3         3  till medical negligence exist government hospi...  Mumbai  2022\n",
       "4         4   doctor reading also government hospital resident  Mumbai  2022"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_cleaned_df = data_preprocessing(tweets_no_dupl_df)\n",
    "print(tweets_cleaned_df.shape)\n",
    "tweets_cleaned_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hinglish_to_english(df, lang):\n",
    "    translated_val = list()\n",
    "        \n",
    "    # translating the text to english using GoogleTranslator API\n",
    "    for x in df['tweet']:\n",
    "        try:\n",
    "            # GoogleTranslator API has a limit of 5000 characters per request, so splitting the text into chunks of 5000 characters and then translating it\n",
    "            if len(str(x))<5000:\n",
    "                \n",
    "                translation = GoogleTranslator(source=lang, target='en').translate(x)\n",
    "                translated_val.append(translation)\n",
    "            \n",
    "            elif len(str(x))>5000 and len(str(x))<10000:\n",
    "                \n",
    "                split_x = [x[i:i+4999] for i in range(0, len(x), 4999)]\n",
    "                translation_1 = GoogleTranslator(source=lang, target='en').translate(split_x[0])\n",
    "                translation_2 = GoogleTranslator(source=lang, target='en').translate(split_x[1])\n",
    "                translation = translation_1 + translation_2\n",
    "                translated_val.append(translation)\n",
    "                \n",
    "        except Exception as e:\n",
    "            # if the text is not in the language provided, then it will return a nan value or text length is more than 15000 characters\n",
    "            translated_val.append(np.nan)\n",
    "    \n",
    "    # replacing the original text with the translated text\n",
    "    df['tweet_translated'] = translated_val\n",
    "    \n",
    "    # returning the updated dataframe\n",
    "    return df\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m translated_df \u001b[39m=\u001b[39m hinglish_to_english(tweets_cleaned_df, \u001b[39m'\u001b[39;49m\u001b[39mhi\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      2\u001b[0m translated_df\u001b[39m.\u001b[39mhead()\n",
      "Cell \u001b[0;32mIn[18], line 10\u001b[0m, in \u001b[0;36mhinglish_to_english\u001b[0;34m(df, lang)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m      7\u001b[0m     \u001b[39m# GoogleTranslator API has a limit of 5000 characters per request, so splitting the text into chunks of 5000 characters and then translating it\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mstr\u001b[39m(x))\u001b[39m<\u001b[39m\u001b[39m5000\u001b[39m:\n\u001b[0;32m---> 10\u001b[0m         translation \u001b[39m=\u001b[39m GoogleTranslator(source\u001b[39m=\u001b[39;49mlang, target\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39men\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49mtranslate(x)\n\u001b[1;32m     11\u001b[0m         translated_val\u001b[39m.\u001b[39mappend(translation)\n\u001b[1;32m     13\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mstr\u001b[39m(x))\u001b[39m>\u001b[39m\u001b[39m5000\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mstr\u001b[39m(x))\u001b[39m<\u001b[39m\u001b[39m10000\u001b[39m:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/Twitter_Data_Analysis-wZ120kVj/lib/python3.10/site-packages/deep_translator/google.py:67\u001b[0m, in \u001b[0;36mGoogleTranslator.translate\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpayload_key:\n\u001b[1;32m     65\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_url_params[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpayload_key] \u001b[39m=\u001b[39m text\n\u001b[0;32m---> 67\u001b[0m response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mget(\n\u001b[1;32m     68\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_base_url, params\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_url_params, proxies\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mproxies\n\u001b[1;32m     69\u001b[0m )\n\u001b[1;32m     70\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m429\u001b[39m:\n\u001b[1;32m     71\u001b[0m     \u001b[39mraise\u001b[39;00m TooManyRequests()\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/Twitter_Data_Analysis-wZ120kVj/lib/python3.10/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39;49m\u001b[39mget\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, params\u001b[39m=\u001b[39;49mparams, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/Twitter_Data_Analysis-wZ120kVj/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/Twitter_Data_Analysis-wZ120kVj/lib/python3.10/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    582\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    583\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    584\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    585\u001b[0m }\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/Twitter_Data_Analysis-wZ120kVj/lib/python3.10/site-packages/requests/sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    700\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    703\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    704\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/Twitter_Data_Analysis-wZ120kVj/lib/python3.10/site-packages/requests/adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    488\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 489\u001b[0m         resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    490\u001b[0m             method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    491\u001b[0m             url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    492\u001b[0m             body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    493\u001b[0m             headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    494\u001b[0m             redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m             assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    496\u001b[0m             preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    497\u001b[0m             decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    498\u001b[0m             retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    499\u001b[0m             timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    500\u001b[0m         )\n\u001b[1;32m    502\u001b[0m     \u001b[39m# Send the request.\u001b[39;00m\n\u001b[1;32m    503\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39mproxy_pool\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/Twitter_Data_Analysis-wZ120kVj/lib/python3.10/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    704\u001b[0m     conn,\n\u001b[1;32m    705\u001b[0m     method,\n\u001b[1;32m    706\u001b[0m     url,\n\u001b[1;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    711\u001b[0m )\n\u001b[1;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[1;32m    717\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/Twitter_Data_Analysis-wZ120kVj/lib/python3.10/site-packages/urllib3/connectionpool.py:386\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[39m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 386\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_conn(conn)\n\u001b[1;32m    387\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    388\u001b[0m     \u001b[39m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n\u001b[1;32m    389\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mconn\u001b[39m.\u001b[39mtimeout)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/Twitter_Data_Analysis-wZ120kVj/lib/python3.10/site-packages/urllib3/connectionpool.py:1042\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[39m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39msock\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):  \u001b[39m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[0;32m-> 1042\u001b[0m     conn\u001b[39m.\u001b[39;49mconnect()\n\u001b[1;32m   1044\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m conn\u001b[39m.\u001b[39mis_verified:\n\u001b[1;32m   1045\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1046\u001b[0m         (\n\u001b[1;32m   1047\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUnverified HTTPS request is being made to host \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1052\u001b[0m         InsecureRequestWarning,\n\u001b[1;32m   1053\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/Twitter_Data_Analysis-wZ120kVj/lib/python3.10/site-packages/urllib3/connection.py:363\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    362\u001b[0m     \u001b[39m# Add certificate verification\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m conn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_new_conn()\n\u001b[1;32m    364\u001b[0m     hostname \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost\n\u001b[1;32m    365\u001b[0m     tls_in_tls \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/Twitter_Data_Analysis-wZ120kVj/lib/python3.10/site-packages/urllib3/connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    171\u001b[0m     extra_kw[\u001b[39m\"\u001b[39m\u001b[39msocket_options\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket_options\n\u001b[1;32m    173\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     conn \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mcreate_connection(\n\u001b[1;32m    175\u001b[0m         (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dns_host, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mport), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mextra_kw\n\u001b[1;32m    176\u001b[0m     )\n\u001b[1;32m    178\u001b[0m \u001b[39mexcept\u001b[39;00m SocketTimeout:\n\u001b[1;32m    179\u001b[0m     \u001b[39mraise\u001b[39;00m ConnectTimeoutError(\n\u001b[1;32m    180\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[1;32m    181\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConnection to \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m timed out. (connect timeout=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    182\u001b[0m         \u001b[39m%\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout),\n\u001b[1;32m    183\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/Twitter_Data_Analysis-wZ120kVj/lib/python3.10/site-packages/urllib3/util/connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[39mif\u001b[39;00m source_address:\n\u001b[1;32m     84\u001b[0m         sock\u001b[39m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 85\u001b[0m     sock\u001b[39m.\u001b[39;49mconnect(sa)\n\u001b[1;32m     86\u001b[0m     \u001b[39mreturn\u001b[39;00m sock\n\u001b[1;32m     88\u001b[0m \u001b[39mexcept\u001b[39;00m socket\u001b[39m.\u001b[39merror \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "translated_df = hinglish_to_english(tweets_cleaned_df, 'hi')\n",
    "translated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.to_csv(translated_df, '/Users/nitanshjain/Documents/Projects/Twitter_Data_Analysis/v2/data/tweets_translated_v2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aadhar</th>\n",
       "      <th>aadmi</th>\n",
       "      <th>aaj</th>\n",
       "      <th>aam</th>\n",
       "      <th>aap</th>\n",
       "      <th>ab</th>\n",
       "      <th>abdul</th>\n",
       "      <th>abhi</th>\n",
       "      <th>abhishek</th>\n",
       "      <th>ability</th>\n",
       "      <th>...</th>\n",
       "      <th>yoga</th>\n",
       "      <th>yogi</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>younger</th>\n",
       "      <th>youth</th>\n",
       "      <th>youtube</th>\n",
       "      <th>yr</th>\n",
       "      <th>zero</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18039</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18040</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18041</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18042</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18043</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18044 rows × 3735 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       aadhar  aadmi  aaj  aam  aap  ab  abdul  abhi  abhishek  ability  ...  \\\n",
       "0           0      0    0    0    0   0      0     0         0        0  ...   \n",
       "1           0      0    0    0    0   0      0     0         0        0  ...   \n",
       "2           0      0    0    0    0   0      0     0         0        0  ...   \n",
       "3           0      0    0    0    0   0      0     0         0        0  ...   \n",
       "4           0      0    0    0    0   0      0     0         0        0  ...   \n",
       "...       ...    ...  ...  ...  ...  ..    ...   ...       ...      ...  ...   \n",
       "18039       0      0    0    0    0   0      0     0         0        0  ...   \n",
       "18040       0      0    0    0    0   0      0     0         0        0  ...   \n",
       "18041       0      0    0    0    0   0      0     0         0        0  ...   \n",
       "18042       0      0    0    0    0   0      0     0         0        0  ...   \n",
       "18043       0      0    0    0    0   0      0     0         0        0  ...   \n",
       "\n",
       "       yoga  yogi  york  young  younger  youth  youtube  yr  zero  zone  \n",
       "0         0     0     0      0        0      0        0   0     0     0  \n",
       "1         0     0     0      0        0      0        0   0     0     0  \n",
       "2         0     0     0      0        0      0        0   0     0     0  \n",
       "3         0     0     0      0        0      0        0   0     0     0  \n",
       "4         0     0     0      0        0      0        0   0     0     0  \n",
       "...     ...   ...   ...    ...      ...    ...      ...  ..   ...   ...  \n",
       "18039     0     0     0      0        0      0        0   0     0     0  \n",
       "18040     0     0     0      0        0      0        0   0     0     0  \n",
       "18041     0     0     0      0        0      0        0   0     0     0  \n",
       "18042     0     0     0      0        0      0        0   0     0     0  \n",
       "18043     0     0     0      0        0      0        0   0     0     0  \n",
       "\n",
       "[18044 rows x 3735 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We are going to create a document-term matrix using CountVectorizer, and exclude common English stop words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(stop_words='english', min_df=10, ngram_range=(1,1))\n",
    "data_clean_cv = cv.fit_transform(translated_df.tweet)\n",
    "# data_clean_cv.toarray()\n",
    "data_clean_dtm = pd.DataFrame(data_clean_cv.toarray(), columns=cv.get_feature_names_out())\n",
    "data_clean_dtm.index = translated_df.index\n",
    "data_clean_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating csv of cleaned dataset\n",
    "pd.DataFrame.to_csv(tweets_cleaned_df, '/Users/nitanshjain/Documents/Projects/Twitter_Data_Analysis/v2/data/tweets_cleaned_with_stopwords_v2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Twitter_Data_Analysis-wZ120kVj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
