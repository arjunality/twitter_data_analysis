{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/arjunkhanchandani/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/arjunkhanchandani/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/arjunkhanchandani/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "import fnmatch\n",
    "import string\n",
    "from urllib.parse import urlparse\n",
    "import contractions\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('words')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, TweetTokenizer\n",
    "from nltk.corpus import words, stopwords\n",
    "from nltk.metrics.distance import jaccard_distance, edit_distance\n",
    "from nltk.util import ngrams\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from wordsegment import load, segment\n",
    "from autocorrect  import Speller\n",
    "\n",
    "#from deep_translator import GoogleTranslator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26015, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>date_created</th>\n",
       "      <th>tweet</th>\n",
       "      <th>city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2022-12-12 16:13:45+00:00</td>\n",
       "      <td>@esichq @byadavbjp @Rameswar_Teli @mygovindia ...</td>\n",
       "      <td>Mumbai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2022-12-10 06:30:56+00:00</td>\n",
       "      <td>@PotholeWarriors @CMOMaharashtra @mieknathshin...</td>\n",
       "      <td>Mumbai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2022-11-23 13:09:18+00:00</td>\n",
       "      <td>@Iam_Ayushmann Govandi is one of the Hotspot o...</td>\n",
       "      <td>Mumbai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2022-10-27 15:58:11+00:00</td>\n",
       "      <td>Till when medical negligence will exist in gov...</td>\n",
       "      <td>Mumbai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2022-07-28 03:03:15+00:00</td>\n",
       "      <td>Me being a doctor reading this\\nAlso governmen...</td>\n",
       "      <td>Mumbai</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0               date_created  \\\n",
       "0           0  2022-12-12 16:13:45+00:00   \n",
       "1           1  2022-12-10 06:30:56+00:00   \n",
       "2           2  2022-11-23 13:09:18+00:00   \n",
       "3           3  2022-10-27 15:58:11+00:00   \n",
       "4           4  2022-07-28 03:03:15+00:00   \n",
       "\n",
       "                                               tweet    city  \n",
       "0  @esichq @byadavbjp @Rameswar_Teli @mygovindia ...  Mumbai  \n",
       "1  @PotholeWarriors @CMOMaharashtra @mieknathshin...  Mumbai  \n",
       "2  @Iam_Ayushmann Govandi is one of the Hotspot o...  Mumbai  \n",
       "3  Till when medical negligence will exist in gov...  Mumbai  \n",
       "4  Me being a doctor reading this\\nAlso governmen...  Mumbai  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df = pd.read_csv('/Users/arjunkhanchandani/Desktop/twitter_data_analysis-main/v2/data/tweets_v2.csv')\n",
    "print(tweets_df.shape)\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet_id         int64\n",
      "date_created    object\n",
      "tweet           object\n",
      "city            object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "tweets_df.rename(columns = {'Unnamed: 0':'tweet_id'}, inplace=True)\n",
    "print(tweets_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>city</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@esichq @byadavbjp @Rameswar_Teli @mygovindia ...</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>@PotholeWarriors @CMOMaharashtra @mieknathshin...</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>@Iam_Ayushmann Govandi is one of the Hotspot o...</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Till when medical negligence will exist in gov...</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Me being a doctor reading this\\nAlso governmen...</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id                                              tweet    city  year\n",
       "0         0  @esichq @byadavbjp @Rameswar_Teli @mygovindia ...  Mumbai  2022\n",
       "1         1  @PotholeWarriors @CMOMaharashtra @mieknathshin...  Mumbai  2022\n",
       "2         2  @Iam_Ayushmann Govandi is one of the Hotspot o...  Mumbai  2022\n",
       "3         3  Till when medical negligence will exist in gov...  Mumbai  2022\n",
       "4         4  Me being a doctor reading this\\nAlso governmen...  Mumbai  2022"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#selecting only year from date_created column\n",
    "tweets_df['date_created'] = pd.to_datetime(tweets_df['date_created'])\n",
    "tweets_df['year'] = tweets_df['date_created'].dt.year\n",
    "tweets_df.drop(['date_created'], axis=1, inplace=True)\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delhi        10005\n",
      "Mumbai        6715\n",
      "Hyderabad     3597\n",
      "Bangalore     3229\n",
      "Kolkata       1413\n",
      "Chennai       1056\n",
      "Name: city, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(tweets_df.city.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020    8963\n",
      "2021    7255\n",
      "2019    3885\n",
      "2022    3214\n",
      "2018    2698\n",
      "Name: year, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(tweets_df.year.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 26015 entries, 0 to 26014\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   tweet_id  26015 non-null  int64 \n",
      " 1   tweet     26015 non-null  object\n",
      " 2   city      26015 non-null  object\n",
      " 3   year      26015 non-null  int64 \n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 813.1+ KB\n"
     ]
    }
   ],
   "source": [
    "tweets_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset before removal of duplicate tweets is (26015, 4)\n",
      "Shape of dataset after removal of duplicate tweets is (19350, 4)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of dataset before removal of duplicate tweets is {}'.format(tweets_df.shape))\n",
    "tweets_no_dupl_df = tweets_df.drop_duplicates(subset=['tweet'])\n",
    "print('Shape of dataset after removal of duplicate tweets is {}'.format(tweets_no_dupl_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delhi        7379\n",
      "Mumbai       4963\n",
      "Hyderabad    2614\n",
      "Bangalore    2471\n",
      "Kolkata      1077\n",
      "Chennai       846\n",
      "Name: city, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(tweets_no_dupl_df.city.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020    6517\n",
      "2021    5515\n",
      "2019    2767\n",
      "2022    2454\n",
      "2018    2097\n",
      "Name: year, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(tweets_no_dupl_df.year.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbs_df = pd.read_csv('/Users/arjunkhanchandani/Desktop/twitter_data_analysis-main/v2/data/text_abbreviations_v2.csv')\n",
    "# abbs_df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "abbs_df.text = abbs_df.text.str.lower()\n",
    "\n",
    "abbs_dict = zip(abbs_df.text, abbs_df.meaning)\n",
    "abbs_dict = list(abbs_dict)\n",
    "abbs_dict = dict(abbs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>city</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@esichq @byadavbjp @Rameswar_Teli @mygovindia ...</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>@PotholeWarriors @CMOMaharashtra @mieknathshin...</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>@Iam_Ayushmann Govandi is one of the Hotspot o...</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Till when medical negligence will exist in gov...</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Me being a doctor reading this\\nAlso governmen...</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25845</th>\n",
       "      <td>25845</td>\n",
       "      <td>@KTRTRS Sir please take action on supraja hosp...</td>\n",
       "      <td>Hyderabad</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25859</th>\n",
       "      <td>25859</td>\n",
       "      <td>* Why meme police didn't waited for his PM rep...</td>\n",
       "      <td>Hyderabad</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25918</th>\n",
       "      <td>25918</td>\n",
       "      <td>@NewBolarum @amksocialwork @TOIHyderabad @Tela...</td>\n",
       "      <td>Hyderabad</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25962</th>\n",
       "      <td>25962</td>\n",
       "      <td>@narendramodi @PMOIndia \\nHonorable PM we are ...</td>\n",
       "      <td>Hyderabad</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25975</th>\n",
       "      <td>25975</td>\n",
       "      <td>Plz look into the deaths of infants &amp;amp; new ...</td>\n",
       "      <td>Hyderabad</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19350 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       tweet_id                                              tweet       city  \\\n",
       "0             0  @esichq @byadavbjp @Rameswar_Teli @mygovindia ...     Mumbai   \n",
       "1             1  @PotholeWarriors @CMOMaharashtra @mieknathshin...     Mumbai   \n",
       "2             2  @Iam_Ayushmann Govandi is one of the Hotspot o...     Mumbai   \n",
       "3             3  Till when medical negligence will exist in gov...     Mumbai   \n",
       "4             4  Me being a doctor reading this\\nAlso governmen...     Mumbai   \n",
       "...         ...                                                ...        ...   \n",
       "25845     25845  @KTRTRS Sir please take action on supraja hosp...  Hyderabad   \n",
       "25859     25859  * Why meme police didn't waited for his PM rep...  Hyderabad   \n",
       "25918     25918  @NewBolarum @amksocialwork @TOIHyderabad @Tela...  Hyderabad   \n",
       "25962     25962  @narendramodi @PMOIndia \\nHonorable PM we are ...  Hyderabad   \n",
       "25975     25975  Plz look into the deaths of infants &amp; new ...  Hyderabad   \n",
       "\n",
       "       year  \n",
       "0      2022  \n",
       "1      2022  \n",
       "2      2022  \n",
       "3      2022  \n",
       "4      2022  \n",
       "...     ...  \n",
       "25845  2021  \n",
       "25859  2020  \n",
       "25918  2020  \n",
       "25962  2021  \n",
       "25975  2020  \n",
       "\n",
       "[19350 rows x 4 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_no_dupl_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1y/mk1sttkj7yddgbjksb454nt00000gn/T/ipykernel_13178/391035932.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets_no_dupl_df['hashtags'] = tweets_no_dupl_df['tweet'].apply(lambda x: re.findall(r'\\B#\\w*[a-zA-Z]+\\w*', x)) #creating a new column\n"
     ]
    }
   ],
   "source": [
    "#extracting hashtags\n",
    "tweets_no_dupl_df['hashtags'] = tweets_no_dupl_df['tweet'].apply(lambda x: re.findall(r'\\B#\\w*[a-zA-Z]+\\w*', x)) #creating a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>city</th>\n",
       "      <th>year</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@esichq @byadavbjp @Rameswar_Teli @mygovindia ...</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2022</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>@PotholeWarriors @CMOMaharashtra @mieknathshin...</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2022</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>@Iam_Ayushmann Govandi is one of the Hotspot o...</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2022</td>\n",
       "      <td>[#Measles]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Till when medical negligence will exist in gov...</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2022</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Me being a doctor reading this\\nAlso governmen...</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2022</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25845</th>\n",
       "      <td>25845</td>\n",
       "      <td>@KTRTRS Sir please take action on supraja hosp...</td>\n",
       "      <td>Hyderabad</td>\n",
       "      <td>2021</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25859</th>\n",
       "      <td>25859</td>\n",
       "      <td>* Why meme police didn't waited for his PM rep...</td>\n",
       "      <td>Hyderabad</td>\n",
       "      <td>2020</td>\n",
       "      <td>[#IndiaFightsBack4SSR]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25918</th>\n",
       "      <td>25918</td>\n",
       "      <td>@NewBolarum @amksocialwork @TOIHyderabad @Tela...</td>\n",
       "      <td>Hyderabad</td>\n",
       "      <td>2020</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25962</th>\n",
       "      <td>25962</td>\n",
       "      <td>@narendramodi @PMOIndia \\nHonorable PM we are ...</td>\n",
       "      <td>Hyderabad</td>\n",
       "      <td>2021</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25975</th>\n",
       "      <td>25975</td>\n",
       "      <td>Plz look into the deaths of infants &amp;amp; new ...</td>\n",
       "      <td>Hyderabad</td>\n",
       "      <td>2020</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19350 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       tweet_id                                              tweet       city  \\\n",
       "0             0  @esichq @byadavbjp @Rameswar_Teli @mygovindia ...     Mumbai   \n",
       "1             1  @PotholeWarriors @CMOMaharashtra @mieknathshin...     Mumbai   \n",
       "2             2  @Iam_Ayushmann Govandi is one of the Hotspot o...     Mumbai   \n",
       "3             3  Till when medical negligence will exist in gov...     Mumbai   \n",
       "4             4  Me being a doctor reading this\\nAlso governmen...     Mumbai   \n",
       "...         ...                                                ...        ...   \n",
       "25845     25845  @KTRTRS Sir please take action on supraja hosp...  Hyderabad   \n",
       "25859     25859  * Why meme police didn't waited for his PM rep...  Hyderabad   \n",
       "25918     25918  @NewBolarum @amksocialwork @TOIHyderabad @Tela...  Hyderabad   \n",
       "25962     25962  @narendramodi @PMOIndia \\nHonorable PM we are ...  Hyderabad   \n",
       "25975     25975  Plz look into the deaths of infants &amp; new ...  Hyderabad   \n",
       "\n",
       "       year                hashtags  \n",
       "0      2022                      []  \n",
       "1      2022                      []  \n",
       "2      2022              [#Measles]  \n",
       "3      2022                      []  \n",
       "4      2022                      []  \n",
       "...     ...                     ...  \n",
       "25845  2021                      []  \n",
       "25859  2020  [#IndiaFightsBack4SSR]  \n",
       "25918  2020                      []  \n",
       "25962  2021                      []  \n",
       "25975  2020                      []  \n",
       "\n",
       "[19350 rows x 5 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_no_dupl_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1y/mk1sttkj7yddgbjksb454nt00000gn/T/ipykernel_13178/2018065208.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets_no_dupl_df['tweet'] = tweets_no_dupl_df['tweet'].apply(lambda x: remove_mentions_hashtags(x))\n"
     ]
    }
   ],
   "source": [
    "#removing mentions\n",
    "def remove_mentions(tweet):\n",
    "    tweet = ' '.join(re.sub(r\"@\\w+\",\" \", tweet).split())\n",
    "    return tweet\n",
    "\n",
    "tweets_no_dupl_df['tweet'] = tweets_no_dupl_df['tweet'].apply(lambda x: remove_mentions(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        It is very bad thing to say that government di...\n",
       "1        all netas and their families should be admitte...\n",
       "2        Govandi is one of the Hotspot of #Measles as w...\n",
       "3        Till when medical negligence will exist in gov...\n",
       "4        Me being a doctor reading this Also government...\n",
       "                               ...                        \n",
       "25845    Sir please take action on supraja hospital nag...\n",
       "25859    * Why meme police didn't waited for his PM rep...\n",
       "25918    Such a good facility with 30 bedded inpatient ...\n",
       "25962    Honorable PM we are aware Private Hospitals ar...\n",
       "25975    Plz look into the deaths of infants &amp; new ...\n",
       "Name: tweet, Length: 19350, dtype: object"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_no_dupl_df['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#segmenting words\n",
    "def segment_words(tweet):\n",
    "    load()\n",
    "    tweet = ' '.join(segment(tweet))\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "tweets_no_dupl_df['tweet'] = tweets_no_dupl_df['tweet'].apply(lambda x: segment_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def autocorrect(tweet):\n",
    "#     spell = Speller(lang='en')\n",
    "#     tweet = ' '.join([spell(w) for w in tweet.split()])\n",
    "#     return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets_no_dupl_df['tweet'] = tweets_no_dupl_df['tweet'].apply(lambda x: autocorrect(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing links\n",
    "def removing_links(df):\n",
    "     for tweets in df.loc[:,'tweet']:\n",
    "          \n",
    "          tokenizer = TweetTokenizer()\n",
    "          tweet_id = df.loc[df['tweet'] == tweets, 'tweet_id'].values[0]\n",
    "        \n",
    "          tokens = tokenizer.tokenize(tweets)\n",
    "          x = [word for word in tokens if not urlparse(word).scheme]\n",
    "          tweets = ' '.join(x)\n",
    "          \n",
    "          df.loc[df['tweet_id']==tweet_id, 'tweet'] = tweets\n",
    "          \n",
    "     return df\n",
    "\n",
    "tweets_no_dupl_df = removing_links(tweets_no_dupl_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_no_dupl_df['tweet'] = tweets_no_dupl_df['tweet'] .str.replace('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_no_dupl_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_text = re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", \"\", tweets_no_dupl_df['tweet'] )\n",
    "tweets_no_dupl_df['tweet']  = \" \".join(processed_text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_no_dupl_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replacing_abbr(df, dictry):\n",
    "    for tweets in df.loc[:,'tweet']:\n",
    "          \n",
    "          tweet_id = df.loc[df['tweet'] == tweets, 'tweet_id'].values[0]\n",
    "          tweets = tweets.split()\n",
    "          tweets = [dictry[word] if word in dictry else word for word in tweets]\n",
    "          tweets = ' '.join(tweets)\n",
    "          df.loc[df['tweet_id']==tweet_id, 'tweet'] = tweets\n",
    "     \n",
    "    return df\n",
    "\n",
    "def contractions_handling(df):\n",
    "     for tweets in df.loc[:,'tweet']:\n",
    "          \n",
    "          tweet_id = df.loc[df['tweet'] == tweets, 'tweet_id'].values[0]\n",
    "          tweets = contractions.fix(tweets)\n",
    "          \n",
    "          df.loc[df['tweet_id']==tweet_id, 'tweet'] = tweets\n",
    "          \n",
    "     return df\n",
    "\n",
    "def adding_space_bw_words_punc(df):\n",
    "     for tweets in df.loc[:, 'tweet']:\n",
    "          \n",
    "          tokenizer = TweetTokenizer()\n",
    "          tweet_id = df.loc[df['tweet'] == tweets, 'tweet_id'].values[0]\n",
    "          tweets = tweets.replace(',', ' , ').replace('.', ' . ').replace('?', ' ? ').replace('!', ' ! ').replace('-', ' - ').replace('(', ' ( ').replace(')', ' ) ').replace(':', ' : ').replace(';', ' ; ').replace('\"', ' \" ').replace(\"'\", \" ' \").replace('  ', ' ')\n",
    "          \n",
    "          df.loc[df['tweet_id']==tweet_id, 'tweet'] = tweets\n",
    "     \n",
    "     return df\n",
    "\n",
    "def removing_hashtags_mentions(df):\n",
    "     for tweets in df.loc[:, 'tweet']:\n",
    "          \n",
    "          tokenizer = TweetTokenizer()\n",
    "          tweet_id = df.loc[df['tweet'] == tweets, 'tweet_id'].values[0]\n",
    "          \n",
    "          tokens = tokenizer.tokenize(tweets)\n",
    "          tokens = [word for word in tokens if word[0] not in ('#', '@')]\n",
    "          tokens = [word for word in tokens if word[0] not in ('▪')]\n",
    "          tweets = ' '.join(tokens)\n",
    "          \n",
    "          df.loc[df['tweet_id']==tweet_id, 'tweet'] = tweets\n",
    "     \n",
    "     return df\n",
    "          \n",
    "def removing_punctuations_emojis(df):\n",
    "     for tweets in df.loc[:, 'tweet']:\n",
    "          \n",
    "          tokenizer = TweetTokenizer()\n",
    "          tweet_id = df.loc[df['tweet'] == tweets, 'tweet_id'].values[0]\n",
    "          \n",
    "          tweets = tweets.translate(str.maketrans(' ', ' ', string.punctuation)) # removes punctuations\n",
    "          tweets = re.sub(r'[^\\x00-\\x7F]+', ' ', tweets) # removes emojis\n",
    "          tweets = tweets.lower() # converts text to lower case\n",
    "          \n",
    "          df.loc[df['tweet_id']==tweet_id, 'tweet'] = tweets\n",
    "     \n",
    "     return df\n",
    "\n",
    "def removing_numbers(df):\n",
    "     for tweets in df.loc[:, 'tweet']:\n",
    "          \n",
    "          tokenizer = TweetTokenizer()\n",
    "          tweet_id = df.loc[df['tweet'] == tweets, 'tweet_id'].values[0]\n",
    "          \n",
    "          #remove numbers\n",
    "          tweets = re.sub(r'\\d+', '', tweets)\n",
    "          tweets = re.sub(' +', ' ', tweets)\n",
    "          \n",
    "          df.loc[df['tweet_id']==tweet_id, 'tweet'] = tweets\n",
    "     \n",
    "     return df\n",
    "\n",
    "def removing_stopwords(df):\n",
    "     for tweets in df.loc[:,'tweet']:\n",
    "          \n",
    "          tokenizer = TweetTokenizer()\n",
    "          tweet_id = df.loc[df['tweet'] == tweets, 'tweet_id'].values[0]\n",
    "          \n",
    "          tokens = tokenizer.tokenize(tweets)\n",
    "          filtered_tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "          tweets = ' '.join(filtered_tokens)\n",
    "          \n",
    "          df.loc[df['tweet_id']==tweet_id, 'tweet'] = tweets\n",
    "          \n",
    "     return df\n",
    "\n",
    "def lemmatization(df):\n",
    "     for tweets in df.loc[:,'tweet']:\n",
    "          \n",
    "          tokenizer = TweetTokenizer()\n",
    "          tweet_id = df.loc[df['tweet'] == tweets, 'tweet_id'].values[0]\n",
    "          \n",
    "          tokens = tokenizer.tokenize(tweets)\n",
    "          lemmatizer = WordNetLemmatizer()\n",
    "          lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "          tweets = ' '.join(lemmatized_tokens)\n",
    "          \n",
    "          df.loc[df['tweet_id']==tweet_id, 'tweet'] = tweets\n",
    "          \n",
    "     return df\n",
    "\n",
    "def removing_characters_less_3(df):\n",
    "     for tweets in df.loc[:,'tweet']:\n",
    "          \n",
    "          tokenizer = TweetTokenizer()\n",
    "          tweet_id = df.loc[df['tweet'] == tweets, 'tweet_id'].values[0]\n",
    "          \n",
    "          tokens = tokenizer.tokenize(tweets)\n",
    "          filtered_tokens = [word for word in tokens if len(word) > 2]\n",
    "          tweets = ' '.join(filtered_tokens)\n",
    "          \n",
    "          df.loc[df['tweet_id']==tweet_id, 'tweet'] = tweets\n",
    "          \n",
    "     return df\n",
    "\n",
    "def removing_tweets_less_5(df):\n",
    "     for tweets in df.loc[:, 'tweet']:\n",
    "          \n",
    "          tweet_id = df.loc[df['tweet'] == tweets, 'tweet_id'].values[0]\n",
    "          \n",
    "          if len(tweets.split()) < 5:\n",
    "               df.drop(df[df['tweet_id']==tweet_id].index, inplace=True)\n",
    "     return df\n",
    "\n",
    "def removing_duplicates(df):\n",
    "     df = df.drop_duplicates(subset=['tweet'])\n",
    "     \n",
    "     for i in range(len(df)-1):\n",
    "          try:\n",
    "               if df['tweet'].iloc[i].split()[:5] == df['tweet'].iloc[i+1].split()[:5]:\n",
    "                    df.drop(df[df['tweet_id']==df['tweet_id'].iloc[i+1]].index, inplace=True)\n",
    "                    i -= 1\n",
    "               else:\n",
    "                    i += 1\n",
    "          except:\n",
    "               continue\n",
    "               \n",
    "     return df\n",
    "\n",
    "\n",
    "def data_preprocessing(df, dictry):\n",
    "     \n",
    "     df = removing_links(df)\n",
    "     df = replacing_abbr(df, dictry)\n",
    "     df = contractions_handling(df)\n",
    "     df = adding_space_bw_words_punc(df)\n",
    "     df = removing_hashtags_mentions(df)\n",
    "     df = removing_punctuations_emojis(df)\n",
    "     df = removing_numbers(df)\n",
    "     df = removing_stopwords(df)\n",
    "     df = lemmatization(df)\n",
    "     df = removing_characters_less_3(df)\n",
    "     df = removing_tweets_less_5(df)\n",
    "     df = removing_duplicates(df)\n",
    "     df.reset_index(drop=True, inplace=True)\n",
    "     \n",
    "     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_cleaned_df = data_preprocessing(tweets_no_dupl_df, abbs_dict)\n",
    "print(tweets_cleaned_df.shape)\n",
    "tweets_cleaned_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating csv of cleaned dataset\n",
    "pd.DataFrame.to_csv(tweets_cleaned_df, '/Users/arjunkhanchandani/Desktop/twitter_data_analysis-main/v2/data/tweets_cleaned_v2.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handing Hinglish\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def hinglish_to_english(df, lang):\n",
    "#     translated_val = list()\n",
    "#     count = 0\n",
    "#     # translating the text to english using GoogleTranslator API\n",
    "#     for x in df['tweet']:\n",
    "#         count += 1\n",
    "#         try:\n",
    "#             # GoogleTranslator API has a limit of 5000 characters per request, so splitting the text into chunks of 5000 characters and then translating it\n",
    "#             if len(str(x))<5000:\n",
    "                \n",
    "#                 translation = GoogleTranslator(source=lang, target='en').translate(x)\n",
    "#                 translated_val.append(translation)\n",
    "            \n",
    "#             elif len(str(x))>5000 and len(str(x))<10000:\n",
    "                \n",
    "#                 split_x = [x[i:i+4999] for i in range(0, len(x), 4999)]\n",
    "#                 translation_1 = GoogleTranslator(source=lang, target='en').translate(split_x[0])\n",
    "#                 translation_2 = GoogleTranslator(source=lang, target='en').translate(split_x[1])\n",
    "#                 translation = translation_1 + translation_2\n",
    "#                 translated_val.append(translation)\n",
    "                \n",
    "#         except Exception as e:\n",
    "#             # if the text is not in the language provided, then it will return a nan value or text length is more than 15000 characters\n",
    "#             translated_val.append(np.nan)\n",
    "#         if count%1000==0:\n",
    "#             print(count)\n",
    "    \n",
    "#     # replacing the original text with the translated text\n",
    "#     df['tweet_translated'] = translated_val\n",
    "    \n",
    "#     # returning the updated dataframe\n",
    "#     return df\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translated_df = hinglish_to_english(tweets_cleaned_df, 'hi')\n",
    "# print(translated_df.isna().sum())\n",
    "# translated_df.dropna(inplace=True)\n",
    "# print(translated_df.isna().sum())\n",
    "# translated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame.to_csv(translated_df, '/Users/nitanshjain/Documents/Projects/Twitter_Data_Analysis/v2/data/tweets_cleaned_translated_v2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Twitter_Data_Analysis-wZ120kVj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5172d4a7dc435df7e7f5965f3153a76d19e76c878c312147a2d42ea606cb17c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
