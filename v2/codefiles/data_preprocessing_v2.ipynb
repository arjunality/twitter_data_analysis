{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "import fnmatch\n",
    "import string\n",
    "from urllib.parse import urlparse\n",
    "import contractions\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('words')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, TweetTokenizer\n",
    "from nltk.corpus import words, stopwords\n",
    "from nltk.metrics.distance import jaccard_distance, edit_distance\n",
    "from nltk.util import ngrams\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from deep_translator import GoogleTranslator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = pd.read_csv('/Users/nitanshjain/Documents/Projects/Twitter_Data_Analysis/v2/data/tweets_v2.csv')\n",
    "print(tweets_df.shape)\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.rename(columns = {'Unnamed: 0':'tweet_id'}, inplace=True)\n",
    "print(tweets_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting only year from date_created column\n",
    "tweets_df['date_created'] = pd.to_datetime(tweets_df['date_created'])\n",
    "tweets_df['year'] = tweets_df['date_created'].dt.year\n",
    "tweets_df.drop(['date_created'], axis=1, inplace=True)\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweets_df.city.value_counts())\n",
    "print(tweets_df.year.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape of dataset before removal of duplicates is {}'.format(tweets_df.shape))\n",
    "tweets_no_dupl_df = tweets_df.drop_duplicates(subset=['tweet'])\n",
    "print('Shape of dataset after removal of duplicates is {}'.format(tweets_no_dupl_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweets_no_dupl_df.city.value_counts())\n",
    "print(tweets_no_dupl_df.year.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def removing_links(df):\n",
    "     for tweets in df.loc[:,'tweet']:\n",
    "          \n",
    "          tokenizer = TweetTokenizer()\n",
    "          tweet_id = df.loc[df['tweet'] == tweets, 'tweet_id'].values[0]\n",
    "        \n",
    "          tokens = tokenizer.tokenize(tweets)\n",
    "          x = [word for word in tokens if not urlparse(word).scheme]\n",
    "          tweets = ' '.join(x)\n",
    "          \n",
    "          df.loc[df['tweet_id']==tweet_id, 'tweet'] = tweets\n",
    "          \n",
    "     return df\n",
    "\n",
    "def contractions_handling(df):\n",
    "     for tweets in df.loc[:,'tweet']:\n",
    "          \n",
    "          tweet_id = df.loc[df['tweet'] == tweets, 'tweet_id'].values[0]\n",
    "          tweets = contractions.fix(tweets)\n",
    "          \n",
    "          df.loc[df['tweet_id']==tweet_id, 'tweet'] = tweets\n",
    "          \n",
    "     return df\n",
    "\n",
    "def adding_space_bw_words_punc(df):\n",
    "     for tweets in df.loc[:, 'tweet']:\n",
    "          \n",
    "          tokenizer = TweetTokenizer()\n",
    "          tweet_id = df.loc[df['tweet'] == tweets, 'tweet_id'].values[0]\n",
    "          tweets = tweets.replace(',', ' , ').replace('.', ' . ').replace('?', ' ? ').replace('!', ' ! ')\n",
    "          \n",
    "          df.loc[df['tweet_id']==tweet_id, 'tweet'] = tweets\n",
    "     \n",
    "     return df\n",
    "\n",
    "def removing_hashtags_mentions(df):\n",
    "     for tweets in df.loc[:, 'tweet']:\n",
    "          \n",
    "          tokenizer = TweetTokenizer()\n",
    "          tweet_id = df.loc[df['tweet'] == tweets, 'tweet_id'].values[0]\n",
    "          \n",
    "          tokens = tokenizer.tokenize(tweets)\n",
    "          tokens = [word for word in tokens if word[0] not in ('#', '@')]\n",
    "          tokens = [word for word in tokens if word[0] not in ('â–ª')]\n",
    "          tweets = ' '.join(tokens)\n",
    "          \n",
    "          df.loc[df['tweet_id']==tweet_id, 'tweet'] = tweets\n",
    "     \n",
    "     return df\n",
    "          \n",
    "def removing_punctuations_emojis(df):\n",
    "     for tweets in df.loc[:, 'tweet']:\n",
    "          \n",
    "          tokenizer = TweetTokenizer()\n",
    "          tweet_id = df.loc[df['tweet'] == tweets, 'tweet_id'].values[0]\n",
    "          \n",
    "          tweets = tweets.translate(str.maketrans('', '', string.punctuation)) # removes punctuations\n",
    "          tweets = re.sub(r'[^\\x00-\\x7F]+', ' ', tweets) # removes emojis\n",
    "          tweets = tweets.lower() # converts text to lower case\n",
    "          \n",
    "          df.loc[df['tweet_id']==tweet_id, 'tweet'] = tweets\n",
    "     \n",
    "     return df\n",
    "\n",
    "def removing_numbers(df):\n",
    "     for tweets in df.loc[:, 'tweet']:\n",
    "          \n",
    "          tokenizer = TweetTokenizer()\n",
    "          tweet_id = df.loc[df['tweet'] == tweets, 'tweet_id'].values[0]\n",
    "          \n",
    "          #remove numbers\n",
    "          tweets = re.sub(r'\\d+', '', tweets)\n",
    "          tweets = re.sub(' +', ' ', tweets)\n",
    "          \n",
    "          df.loc[df['tweet_id']==tweet_id, 'tweet'] = tweets\n",
    "     \n",
    "     return df\n",
    "\n",
    "def removing_stopwords(df):\n",
    "     for tweets in df.loc[:,'tweet']:\n",
    "          \n",
    "          tokenizer = TweetTokenizer()\n",
    "          tweet_id = df.loc[df['tweet'] == tweets, 'tweet_id'].values[0]\n",
    "          \n",
    "          tokens = tokenizer.tokenize(tweets)\n",
    "          filtered_tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "          tweets = ' '.join(filtered_tokens)\n",
    "          \n",
    "          df.loc[df['tweet_id']==tweet_id, 'tweet'] = tweets\n",
    "          \n",
    "     return df\n",
    "\n",
    "def lemmatization(df):\n",
    "     for tweets in df.loc[:,'tweet']:\n",
    "          \n",
    "          tokenizer = TweetTokenizer()\n",
    "          tweet_id = df.loc[df['tweet'] == tweets, 'tweet_id'].values[0]\n",
    "          \n",
    "          tokens = tokenizer.tokenize(tweets)\n",
    "          lemmatizer = WordNetLemmatizer()\n",
    "          lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "          tweets = ' '.join(lemmatized_tokens)\n",
    "          \n",
    "          df.loc[df['tweet_id']==tweet_id, 'tweet'] = tweets\n",
    "          \n",
    "     return df\n",
    "\n",
    "def removing_single_characters(df):\n",
    "     for tweets in df.loc[:,'tweet']:\n",
    "          \n",
    "          tokenizer = TweetTokenizer()\n",
    "          tweet_id = df.loc[df['tweet'] == tweets, 'tweet_id'].values[0]\n",
    "          \n",
    "          tokens = tokenizer.tokenize(tweets)\n",
    "          filtered_tokens = [word for word in tokens if len(word) > 1]\n",
    "          tweets = ' '.join(filtered_tokens)\n",
    "          \n",
    "          df.loc[df['tweet_id']==tweet_id, 'tweet'] = tweets\n",
    "          \n",
    "     return df\n",
    "\n",
    "def removing_tweets_less_5(df):\n",
    "     for tweets in df.loc[:, 'tweet']:\n",
    "          \n",
    "          tweet_id = df.loc[df['tweet'] == tweets, 'tweet_id'].values[0]\n",
    "          \n",
    "          if len(tweets.split()) < 5:\n",
    "               df.drop(df[df['tweet_id']==tweet_id].index, inplace=True)\n",
    "     return df\n",
    "\n",
    "def removing_duplicates(df):\n",
    "     df = df.drop_duplicates(subset=['tweet'])\n",
    "     return df\n",
    "\n",
    "def data_preprocessing(df):\n",
    "     \n",
    "     df = removing_links(df)\n",
    "     df = contractions_handling(df)\n",
    "     df = adding_space_bw_words_punc(df)\n",
    "     df = removing_hashtags_mentions(df)\n",
    "     df = removing_punctuations_emojis(df)\n",
    "     df = removing_numbers(df)\n",
    "     df = lemmatization(df)\n",
    "     df = removing_stopwords(df)\n",
    "     df = removing_single_characters(df)\n",
    "     df = removing_tweets_less_5(df)\n",
    "     df = removing_duplicates(df)\n",
    "     df.reset_index(drop=True, inplace=True)\n",
    "     \n",
    "     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_cleaned_df = data_preprocessing(tweets_no_dupl_df)\n",
    "print(tweets_cleaned_df.shape)\n",
    "tweets_cleaned_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Abbreviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>meaning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>;S</td>\n",
       "      <td>Gentle warning, like \"Hmm? What did you say?\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>?</td>\n",
       "      <td>I have a question</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>?</td>\n",
       "      <td>I don't understand what you mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>?4U</td>\n",
       "      <td>I have a question for you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.02</td>\n",
       "      <td>My (or your) two cents worth</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  text                                        meaning\n",
       "0   ;S  Gentle warning, like \"Hmm? What did you say?\"\n",
       "1    ?                              I have a question\n",
       "2    ?               I don't understand what you mean\n",
       "3  ?4U                      I have a question for you\n",
       "4  .02                   My (or your) two cents worth"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abbs_df = pd.read_csv('/Users/nitanshjain/Documents/Projects/Twitter_Data_Analysis/v2/data/text_abbreviations_v2.csv')\n",
    "abbs_df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "abbs_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handing Hinglish\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hinglish_to_english(df, lang):\n",
    "    translated_val = list()\n",
    "    count = 0\n",
    "    # translating the text to english using GoogleTranslator API\n",
    "    for x in df['tweet']:\n",
    "        count += 1\n",
    "        try:\n",
    "            # GoogleTranslator API has a limit of 5000 characters per request, so splitting the text into chunks of 5000 characters and then translating it\n",
    "            if len(str(x))<5000:\n",
    "                \n",
    "                translation = GoogleTranslator(source=lang, target='en').translate(x)\n",
    "                translated_val.append(translation)\n",
    "            \n",
    "            elif len(str(x))>5000 and len(str(x))<10000:\n",
    "                \n",
    "                split_x = [x[i:i+4999] for i in range(0, len(x), 4999)]\n",
    "                translation_1 = GoogleTranslator(source=lang, target='en').translate(split_x[0])\n",
    "                translation_2 = GoogleTranslator(source=lang, target='en').translate(split_x[1])\n",
    "                translation = translation_1 + translation_2\n",
    "                translated_val.append(translation)\n",
    "                \n",
    "        except Exception as e:\n",
    "            # if the text is not in the language provided, then it will return a nan value or text length is more than 15000 characters\n",
    "            translated_val.append(np.nan)\n",
    "        if count%1000==0:\n",
    "            print(count)\n",
    "    \n",
    "    # replacing the original text with the translated text\n",
    "    df['tweet_translated'] = translated_val\n",
    "    \n",
    "    # returning the updated dataframe\n",
    "    return df\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_df = hinglish_to_english(tweets_cleaned_df, 'hi')\n",
    "translated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.to_csv(translated_df, '/Users/nitanshjain/Documents/Projects/Twitter_Data_Analysis/v2/data/tweets_translated_v2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to create a document-term matrix using CountVectorizer, and exclude common English stop words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(stop_words='english', min_df=10, ngram_range=(1,1))\n",
    "data_clean_cv = cv.fit_transform(translated_df.tweet)\n",
    "# data_clean_cv.toarray()\n",
    "data_clean_dtm = pd.DataFrame(data_clean_cv.toarray(), columns=cv.get_feature_names_out())\n",
    "data_clean_dtm.index = translated_df.index\n",
    "data_clean_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating csv of cleaned dataset\n",
    "pd.DataFrame.to_csv(tweets_cleaned_df, '/Users/nitanshjain/Documents/Projects/Twitter_Data_Analysis/v2/data/tweets_cleaned_with_stopwords_v2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Twitter_Data_Analysis-wZ120kVj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
