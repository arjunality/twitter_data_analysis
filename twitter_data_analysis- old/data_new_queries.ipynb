{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/arjunkhanchandani/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/arjunkhanchandani/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/arjunkhanchandani/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "import fnmatch\n",
    "import string\n",
    "from urllib.parse import urlparse\n",
    "import contractions\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('words')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, TweetTokenizer\n",
    "from nltk.corpus import words, stopwords\n",
    "from nltk.metrics.distance import jaccard_distance, edit_distance\n",
    "from nltk.util import ngrams\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>username</th>\n",
       "      <th>location</th>\n",
       "      <th>following</th>\n",
       "      <th>followers</th>\n",
       "      <th>twt_created_at</th>\n",
       "      <th>total_tweets</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>mentions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>GopinathNagend1</td>\n",
       "      <td>Hyderabad, India</td>\n",
       "      <td>1080</td>\n",
       "      <td>304</td>\n",
       "      <td>2022-11-08 22:28:05+00:00</td>\n",
       "      <td>61966</td>\n",
       "      <td>0</td>\n",
       "      <td>@isvelan @PrapancaMAnudan Why not ? When @Arvi...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'screen_name': 'isvelan', 'name': 'üáÆüá≥ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>pspatilsbi</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>325</td>\n",
       "      <td>25</td>\n",
       "      <td>2022-11-08 22:08:44+00:00</td>\n",
       "      <td>2704</td>\n",
       "      <td>0</td>\n",
       "      <td>This was, this is and....this will be the agen...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'screen_name': 'INCIndia', 'name': 'Congress...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>ththegde</td>\n",
       "      <td>Kandivali East, Mumbai</td>\n",
       "      <td>582</td>\n",
       "      <td>57</td>\n",
       "      <td>2022-11-08 22:00:49+00:00</td>\n",
       "      <td>1969</td>\n",
       "      <td>0</td>\n",
       "      <td>@PMOIndia @nsitharaman please allow citizens t...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'screen_name': 'PMOIndia', 'name': 'PMO Indi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>ahmed_chowdhary</td>\n",
       "      <td>Andheri West, Mumbai</td>\n",
       "      <td>30</td>\n",
       "      <td>21</td>\n",
       "      <td>2022-11-08 21:59:32+00:00</td>\n",
       "      <td>332</td>\n",
       "      <td>0</td>\n",
       "      <td>@ANI Downfall of #IndianEconomy I felt In #Mumbai</td>\n",
       "      <td>[{'text': 'IndianEconomy', 'indices': [17, 31]...</td>\n",
       "      <td>[{'screen_name': 'ANI', 'name': 'ANI', 'id': 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>rupz_boruah</td>\n",
       "      <td>Chabua, India</td>\n",
       "      <td>14</td>\n",
       "      <td>33</td>\n",
       "      <td>2022-11-08 21:54:37+00:00</td>\n",
       "      <td>309</td>\n",
       "      <td>0</td>\n",
       "      <td>Please take necessary action against neurologi...</td>\n",
       "      <td>[{'text': 'Dr_Dhrubajyoti_Kurmi', 'indices': [...</td>\n",
       "      <td>[{'screen_name': 'MoHFW_INDIA', 'name': 'Minis...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  tweet_id         username                location  following  \\\n",
       "0        1         1  GopinathNagend1        Hyderabad, India       1080   \n",
       "1        2         2       pspatilsbi              Bangalore         325   \n",
       "2        3         3         ththegde  Kandivali East, Mumbai        582   \n",
       "3        4         4  ahmed_chowdhary    Andheri West, Mumbai         30   \n",
       "4        5         5      rupz_boruah           Chabua, India         14   \n",
       "\n",
       "   followers             twt_created_at  total_tweets  retweet_count  \\\n",
       "0        304  2022-11-08 22:28:05+00:00         61966              0   \n",
       "1         25  2022-11-08 22:08:44+00:00          2704              0   \n",
       "2         57  2022-11-08 22:00:49+00:00          1969              0   \n",
       "3         21  2022-11-08 21:59:32+00:00           332              0   \n",
       "4         33  2022-11-08 21:54:37+00:00           309              0   \n",
       "\n",
       "                                                text  \\\n",
       "0  @isvelan @PrapancaMAnudan Why not ? When @Arvi...   \n",
       "1  This was, this is and....this will be the agen...   \n",
       "2  @PMOIndia @nsitharaman please allow citizens t...   \n",
       "3  @ANI Downfall of #IndianEconomy I felt In #Mumbai   \n",
       "4  Please take necessary action against neurologi...   \n",
       "\n",
       "                                            hashtags  \\\n",
       "0                                                 []   \n",
       "1                                                 []   \n",
       "2                                                 []   \n",
       "3  [{'text': 'IndianEconomy', 'indices': [17, 31]...   \n",
       "4  [{'text': 'Dr_Dhrubajyoti_Kurmi', 'indices': [...   \n",
       "\n",
       "                                            mentions  \n",
       "0  [{'screen_name': 'isvelan', 'name': 'üáÆüá≥ ...  \n",
       "1  [{'screen_name': 'INCIndia', 'name': 'Congress...  \n",
       "2  [{'screen_name': 'PMOIndia', 'name': 'PMO Indi...  \n",
       "3  [{'screen_name': 'ANI', 'name': 'ANI', 'id': 3...  \n",
       "4  [{'screen_name': 'MoHFW_INDIA', 'name': 'Minis...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df = pd.read_csv('/Users/arjunkhanchandani/Desktop/twitter_data_analysis-main/data/20221109_054000_tweets.csv')\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2811, 12)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset before removal of duplicates is (961, 12)\n",
      "Shape of dataset after removal of duplicates is (961, 12)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of dataset before removal of duplicates is {}'.format(tweets_df.shape))\n",
    "tweets_df.drop_duplicates(subset=['text'], inplace=True)\n",
    "print('Shape of dataset after removal of duplicates is {}'.format(tweets_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id            int64\n",
       "tweet_id           int64\n",
       "username          object\n",
       "location          object\n",
       "following          int64\n",
       "followers          int64\n",
       "twt_created_at    object\n",
       "total_tweets       int64\n",
       "retweet_count      int64\n",
       "text              object\n",
       "hashtags          object\n",
       "mentions          object\n",
       "dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(df):\n",
    "    \"\"\"\n",
    "    One function to rule them all, \n",
    "    one function to find them, \n",
    "    One function to bring them all, \n",
    "    and in the darkness bind them; \n",
    "    \"\"\"\n",
    "    print('Shape of dataset before removal of tweets with less than 5 words is {}'.format(df.shape))\n",
    "    \n",
    "    for tweets in df.loc[:,'text']:\n",
    "        # count+=1\n",
    "        # print(tweets)\n",
    "        tokenizer = TweetTokenizer()\n",
    "        tweet_id = df.loc[df['text'] == tweets, 'tweet_id'].values[0]\n",
    "        # print(tweet_id)\n",
    "        \n",
    "        # removing links\n",
    "        list_words = tokenizer.tokenize(tweets)\n",
    "        x = [word for word in list_words if not urlparse(word).scheme]\n",
    "        tweets = ' '.join(x)\n",
    "\n",
    "        # contractions handling\n",
    "        list_words = tokenizer.tokenize(tweets)\n",
    "        new_list_words = []\n",
    "        for word in list_words:\n",
    "            new_list_words.append(contractions.fix(word))\n",
    "        list_words = new_list_words\n",
    "        del(new_list_words)\n",
    "        tweets = ' '.join(list_words)\n",
    "        \n",
    "        # adding space between words and punctuations\n",
    "        tweets = tweets.replace(',', ' ,').replace('.', ' .').replace('?', ' ?').replace('!', ' !')\n",
    "        \n",
    "        # removing hashtags and mentions\n",
    "        list_words = tokenizer.tokenize(tweets)\n",
    "        list_words = [word for word in list_words if word[0] not in ('#', '@')]\n",
    "        list_words = [word for word in list_words if word[0] not in ('▪')]\n",
    "        tweets = ' '.join(list_words)\n",
    "        \n",
    "        # removing punctuations\n",
    "        tweets = tweets.translate(str.maketrans('', '', string.punctuation))\n",
    "        \n",
    "        #removing emojis\n",
    "        tweets = re.sub(r'[^\\x00-\\x7F]+', ' ', tweets)\n",
    "        \n",
    "        #lower case\n",
    "        tweets = tweets.lower()\n",
    "        \n",
    "        #remove numbers\n",
    "        tweets = re.sub(r'\\d+', '', tweets)\n",
    "        tweets = re.sub(' +', ' ', tweets)\n",
    "        \n",
    "        #removing stopwords\n",
    "        list_words = tokenizer.tokenize(tweets)\n",
    "        filtered_words = [word for word in list_words if word not in stopwords.words('english')]\n",
    "        tweets = ' '.join(filtered_words)\n",
    "        del(filtered_words)\n",
    "        \n",
    "        #lemmatization\n",
    "        lem = WordNetLemmatizer()\n",
    "        list_words = tokenizer.tokenize(tweets)\n",
    "        for word in list_words:\n",
    "            list_words = list(map(lambda x: x.replace(word, lem.lemmatize(word)), list_words))\n",
    "        tweets = ' '.join(list_words)\n",
    "        \n",
    "        #removing individual letters\n",
    "        list_words = tokenizer.tokenize(tweets)\n",
    "        filtered_words = [word for word in list_words if len(word)>2]\n",
    "        tweets = ' '.join(filtered_words)\n",
    "        del(filtered_words)\n",
    "        \n",
    "        # updating tweets in dataframe\n",
    "        df.loc[df['tweet_id']==tweet_id, 'text'] = tweets\n",
    "        \n",
    "        #remove small tweets\n",
    "        list_words = tokenizer.tokenize(tweets)\n",
    "        if len(list_words) <= 5:\n",
    "            ind_num = df[df['tweet_id']==tweet_id].index\n",
    "            df.drop(ind_num, inplace=True)\n",
    "        # break\n",
    "    print('Shape of dataset after removal of tweets with less than 5 words is {}'.format(df.shape))\n",
    "    \n",
    "    return df\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset before removal of tweets with less than 5 words is (961, 12)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [53], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tweets_df \u001b[39m=\u001b[39m data_preprocessing(tweets_df)\n",
      "Cell \u001b[0;32mIn [52], line 14\u001b[0m, in \u001b[0;36mdata_preprocessing\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m tweets \u001b[39min\u001b[39;00m df\u001b[39m.\u001b[39mloc[:,\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m     11\u001b[0m     \u001b[39m# count+=1\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[39m# print(tweets)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     tokenizer \u001b[39m=\u001b[39m TweetTokenizer()\n\u001b[0;32m---> 14\u001b[0m     tweet_id \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39;49mloc[df[\u001b[39m'\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m==\u001b[39;49m tweets, \u001b[39m'\u001b[39;49m\u001b[39mtweet_id\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mvalues[\u001b[39m0\u001b[39;49m]\n\u001b[1;32m     15\u001b[0m     \u001b[39m# print(tweet_id)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \n\u001b[1;32m     17\u001b[0m     \u001b[39m# removing links\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     list_words \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mtokenize(tweets)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "tweets_df = data_preprocessing(tweets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting tweets_df into a csv file\n",
    "filename = '/Users/arjunkhanchandani/Desktop/twitter_data_analysis-main/data/final_dataset.csv'\n",
    "tweets_df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
